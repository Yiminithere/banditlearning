\documentclass[10pt,a4article]{amsart}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd,graphicx}
\usepackage{mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[latin2]{inputenc}
\usepackage{t1enc}
\usepackage[mathscr]{eucal}
\usepackage{indentfirst}
\usepackage{graphics}
\usepackage{pict2e}
\usepackage{epic}
\numberwithin{equation}{section}
\usepackage[margin=2.9cm]{geometry}
\usepackage{epstopdf}
\usepackage[dvipsnames]{xcolor}
\usepackage{CJK}
\usepackage[colorlinks, linkcolor = purple, citecolor=blue]{hyperref}


\usepackage[backend=biber,date=year,giveninits=true,sorting=nyt,style=alphabetic,natbib=true,maxcitenames=2,maxbibnames=10,url=false,doi=true,backref=false]{biblatex}

\addbibresource{sb.bib}


\renewbibmacro{in:}{\ifentrytype{article}{}{\printtext{\bibstring{in}\intitlepunct}}}
\renewcommand*{\bibfont}{\small}


\newcommand{\note}[1]{{\leavevmode\color{BrickRed}{#1}}}

 
%<<<<<<< HEAD
 
%=======
\usepackage[colorlinks,linkcolor = purple, citecolor=blue]{hyperref} 
\bibliography{sb}


%>>>>>>> baf27d0de4aef4e5eb782743d2f58d27984505ef

\theoremstyle{plain}
\newtheorem{Th}{Theorem}
\newtheorem{Lemma}[Th]{Lemma}
\newtheorem{Cor}[Th]{Corollary}
\newtheorem{Prop}[Th]{Proposition}


\theoremstyle{definition}
\newtheorem{Def}[Th]{Definition}
\newtheorem{Conj}[Th]{Conjecture}
\newtheorem{Rem}[Th]{Remark}
\newtheorem{?}[Th]{Problem}
\newtheorem{Ex}[Th]{Example}


\def\R{{\mathbb R}}
\def\Q{{\mathbb Q}}
\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\E{{\mathbb E}}
\def\R{{\mathbb R}}
\def\Y{{\mathcal Y}}
\def\L{{\mathcal L}}
\def\H{{\mathcal H}}
\def\D{{\mathcal D}}
\def\P{{\mathbb P}}
\def\M{{\mathbb M}}
\def\V{{\mathcal V}}
\def\S{{\mathbb S}}
\def\A{{\mathbf A}}
\def\x{{\mathbf x}}
\def\b{{\mathbf b}}
\def\a{{\mathbf a}}
\def\Ph{{\mathbf {\Phi}}}

\def\h{{\mathbf{h}}}
\def\G{{\Gamma}}
\def\s{{\sigma}}
\def\e{{\varepsilon}}
\def\l{{\lambda}}
\def\p{{\phi}}
\def\v{{\mathbf{v}}}
\def\t{{\theta}}
\def\z{{\zeta}}
\def\o{{\omega}}
\def\y{{\mathbf{y}}}
\def\g{{\mathbf{g}}}
\def\u{{\mathbf{u}}}
\def\w{{\mathbf{w}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{filecontents}

\usepackage{CJK}
\usepackage[colorlinks]{hyperref}

\usepackage{mathtools}
\usepackage{commath}
\usepackage[dvipsnames]{xcolor}





\begin{document}

\title{\textbf{Notes on bandit learning}}
\date{}
\author{Yiming Xu}
\maketitle

These notes are based on the book \cite{lattimore2018bandit}. The purpose is mainly to assist my own understanding of the material, but also keeps a record for what has been covered in the reading course. Any imprecision or mistakes are due to my own. 

\bigskip

\bigskip

\bigskip

\tableofcontents
\newpage

\section{Stochastic bandits}\label{sec1}

\subsection{Problem set-up}

This section provides a brief introduction to the stochastic bandits. Particularly, we will introduce the set-up of the problem as well as a few related algorithms. The discussion is based on the chapters $6$-$10$ and $13$-$16$ in \cite{lattimore2018bandit}.

Suppose that you are facing a $k$-armed slot machine. Every time you can choose one arm to play and the corresponding reward will be immediately revealed. Rewards of the arms not played in the round are assumed hidden. In the context of stochastic bandits, rewards generated by each arm are iid random variables. Suppose that you will play $n$ rounds in total. The goal of bandit learning, roughly speaking, is to find a policy under which the regret is asymptotically as small as possible. The rigorous definitions of a policy and the regret are yet to be articulated, it can be felt that a good policy should strike a balance between exploration of the underplayed arms and exploitation of the arms giving high rewards in the past. The rule of thumb, as we will see, is to embrace optimism when faced with uncertainty. 

Let $A=[k]$ be the set of arms and $n$ be the \emph{horizon} of the game. $\{x_{ti}\}_{t\in [n]}$ denote the sequential rewards generated by arm $i$ in $n$ rounds, which are iid random variables with distribution $\nu_i$ and mean $\mu_i$. Here we assume that $\nu_i$ are $1$-\emph{subgaussian} and \emph{unstructured}, in the sense that acquiring knowledge on one arm does not imply the others. A counterexample is $\mu_2=1-\mu_1$ when $k= 2$. A \emph{deterministic policy} is defined as $\pi = (\pi_t)_{t\in [n]}\in [k]^n$ such that $\pi_t$ is a measurable function of $\{x_{t'\pi_{t'}}\}_{t'\in [t-1]}$. A \emph{non-deterministic policy}, on the other hand, possesses extra randomness conditional on the history. Their difference will be illustrated when we discuss adversarial bandits in future sections. For now, we will mainly focus on the deterministic strategies for stochastic bandits. Intuitively, this means that which arm to play in round $t$ is determined by the information disclosed before $t$. The \emph{regret} $R_n(\pi)$ of $\pi$ is defined as the average difference between rewards collected under $\pi$ and the best possible arm:
\begin{align}
R_n(\pi):=\max_{i\in [k]}\E\left[\sum_{t\in [n]}(x_{ti}-x_{t\pi_t})\right]=\max_{i\in [k]}\mu_i -\E\left[\sum_{t\in [n]}x_{t\pi_t}\right]. \label{s:1}
\end{align}
Here the randomness within the expectation comes from $\{x_{ti}\}_{i\in [k], t\in [n]}$.

A few things to note. Whether the regret defined in \eqref{s:1} is meaningful needs further clarification. Also, it is not clear what kind of bounds would imply that a policy $\pi$ is good. The first question is hard to justify from a mathematical point of view. We may consider \eqref{s:1} as a good place to start with when studying stochastic bandits.  For the second question, note that the trivial policy by selecting a fixed arm leads to a regret which grows linearly in $n$.  Therefore, any policy achieving sublinear regret would be reasonable, and how far this can go will be explored later in this section.

In the following analysis, without loss of generality we assume that arm $1$ is optimal. For $i\in [k]$, define the \emph{suboptimality gap} between $i$ and $1$ as $\Delta_i=\mu_1-\mu_i$. For any $t\leq n$, define the number of rounds where $i$ is chosen before $t$ under $\pi$ as $T_i(t; \pi)$, which is a random quantity adapted to the natural filtration. Using the tower property of expectation, one can rewrite $R_n(\pi)$ as 
\begin{align}
R_n(\pi)=\sum_{i\in [k]}\Delta_i\E[T_i(n; \pi)].\label{s:2}
\end{align} 
Such form provides a convenient way to analyze $R_n$.  Indeed, since the summation is over $i$, one often only needs to bound $\E[T_i(n; \pi)]$ for each $i$ under a given policy.  


\subsection{Algorithms}

We mention two well-known algorithms in stochastic bandit learning: The Explore-then-Commit (ETC) algorithm and the Upper Confidence Bound (UCB) algorithm, each of which is followed by a short summary of their pros and cons, as well as some asymptotic results.   


\begin{algorithm}
\DontPrintSemicolon
 \KwIn{$m$: number of exploration on each arm.}
 \KwOut{ $\pi=(\pi_t)_{t\in [n]}$}
 \While{$t\leq km$}{
\begin{align*}
\pi_t &=  \lceil t\bmod {k}\rceil\\
t &= t +1.
\end{align*}}
\While{$km<t\leq n$}{
\begin{align*}
\pi_t = \arg\max_{i\in [k]}\frac{1}{m}\sum_{t = 1}^{mk}x_{t\pi_t}\mathbb{I}\{\pi_t = i\}
\end{align*}
}
\caption{The Explore-then-Commit Algorithm. } 
\label{alg:ETC}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon
 \KwIn{$\delta$: confidence level.}
 \KwOut{ $\pi=(\pi_t)_{t\in [n]}$}
 \While{$t\leq n$}{
\begin{align*}
\pi_t = \arg\max_{i\in [k]}\text{UCB}_i(t-1, \delta),
\end{align*}
where for $i\in [k]$,
\begin{align*}
\text{UCB}_i(t-1, \delta) =\begin{cases} 
      \infty & T_i(t-1)=0 \\
       \displaystyle\frac{1}{T_i(t-1)}\sum_{s\in [t-1]}x_{s\pi_s}\mathbb{I}\{\pi_s = i\}+\sqrt{\frac{2\log(1/\delta)}{T_i(t-1)}} & T_i(t-1)>0 \\
   \end{cases}
\end{align*}}
\caption{The Upper Confidence Bound Algorithm. } 
\label{alg:UCB}
\end{algorithm}

The idea behind Algorithm \ref{alg:ETC} is simple. Divide $n$ rounds into two parts: first $mk$ rounds for exploration and the remaining time for exploitation. One must settle for a trade-off between these two. If $m$ is small, there is a considerable chance that exploration is poor, resulting in the exploitation procedure sub-optimal.  If $m$ is large, the regret generated in the exploration process will likely dominate. Therefore, the best $m$ is usually set at some middle point, as summarized in the following theorem. 

\begin{Th}\label{thm:ETC}
The regret $R_n$ under ETC is given by
\begin{align}
R_n\leq m\sum_{i\in [k]}\Delta_i + (n-mk)\sum_{i\in [k]}\Delta_i e^{-m\Delta_i^2/4}.
\end{align}
Particularly, when $k=2$, taking $m=\max\left\{1, 4\Delta_2^{-2}\log(n\Delta_2^2/4)\right\}$ yields
\begin{align}
R_n\leq\Delta_2+C\sqrt{n},\label{ETC:opt}
\end{align}
where $C$ is some absolute constant. 
\end{Th}

Theorem \ref{thm:ETC} follows from the tail bounds for subgaussian random variables. There are a few things worth remarking here. Firstly, a high-probabilistic version of the result on the \emph{pseudo-regret} $\bar{R}_n$ (which is defined as the expectation conditional on the policy) can be obtained similarly:
\begin{align*}
\P\left(\bar{R}_n:=n\mu_1-\sum_{t\in [n]}\mu_{\pi_t}\leq m\sum_{i\in [k]}\Delta_i\right)\geq 1-\sum_{i\in [k]}e^{-m\Delta_i^2/4}. 
\end{align*} 
Secondly, despite the fact that \eqref{ETC:opt} gives an optimal bound on regret (which will be specified later), how to achieve it depends on knowledge of both the suboptimality gaps $\Delta_i$ and the horizon $n$. These quantities are usually fixed but may not be revealed to the player in advance. In theory, it can be shown that for two-armed bandits the dependence on $\Delta_i$ can be removed while obtaining a sub-optimal regret bound $n^{2/3}$, and the dependence on $n$ can be resolved by a doubling trick without increasing the regret two much. To address the dependence on the suboptimality gaps, another algorithm called UCB has been proposed, see Algorithm \ref{alg:UCB}. 

%\begin{algorithm}
%\DontPrintSemicolon
% \KwIn{$\delta$: confidence level.}
% \KwOut{ $\pi=(\pi_t)_{t\in [n]}$}
% \While{$t\leq n$}{
%\begin{align*}
%\pi_t = \arg\max_{i\in [k]}\text{UCB}_i(t-1, \delta),
%\end{align*}
%where for $i\in [k]$,
%\begin{align*}
%\text{UCB}_i(t-1, \delta) =\begin{cases} 
%      \infty & T_i(t-1)=0 \\
%       \displaystyle\frac{1}{T_i(t-1)}\sum_{s\in [t-1]}x_{s\pi_s}\mathbb{I}\{\pi_s = i\}+\sqrt{\frac{2\log(1/\delta)}{T_i(t-1)}} & T_i(t-1)>0 \\
%   \end{cases}
%\end{align*}}
%\caption{The Upper Confidence Bound Algorithm. } 
%\label{alg:UCB}
%\end{algorithm}
Break ties equally if there are more than one maximizer. Algorithm \ref{alg:UCB} first explores all arms exactly once, then estimates each arm using the (sample-mean based) upper bound of its $\delta$-confidence interval obtained from the Hoeffding inequality. Intuitively, the arm chosen in round $t$ either has a large sample mean or is underexplored compared to other arms. A suboptimal arm is unlikely to be played long since its optimism bonus is decreasing to zero. The key ingredient lies in choosing a good confidence level $\delta$, which again balances the trade-off between exploration and exploitation.  
\begin{Th}\label{thm:UCB}
Set $\delta=n^{-2}$. The regret under UCB is given by
\begin{align*}
R_n\leq 3\sum_{i\in [k]}\Delta_i +\sum_{i: \Delta_i>0}\frac{16\log n}{\Delta_i}.
\end{align*}
\end{Th}
\begin{proof}
Let $c\in (0,1)$ be a fixed number. Consider the intersection of two events: 
\begin{itemize}
\item UCB overestimates arm $1$ in all $n$ rounds; 
\item UCB for each suboptimal arm $i$ falls below $\mu_1$ at time $t_i$, where $t_i$ is chosen such that $(1-c)\Delta_i\geq\sqrt{2\log(1/\delta)/t_i}$. 
\end{itemize}
It is easy to check that this intersected event occurs with probability at least $1-n\delta-\sum_{i: \Delta_i>0}e^{-t_ic^2\Delta^2_i/2}$. Applying the total probability formula, one obtains an upper bound for $R_n$ using the decomposition form \eqref{s:1}:
\begin{align*}
R_n\leq n\left(n\delta+\sum_{i: \Delta_i>0}e^{-t_ic^2\Delta^2_i/2}\right)+\sum_{i\in [k]}\Delta_i t_i.
\end{align*}
Setting $\delta=n^{-2}$ and $c=1/2$ yields the desired result.   
\end{proof}
Theorem \ref{thm:UCB} can be loose when $\Delta_i$ is small. To settle this issue, separate the  arms into two parts: those with suboptimality gap less than $\sqrt{16k\log n/n}$ and greater than $\sqrt{16k\log n/n}$. Bounding $\E[T_i(n)]$ by $n$ in the first part and by Theorem \ref{thm:UCB} in the second gives
\begin{align}
R_n\leq 3\sum_{i\in [k]}\Delta_i + 8\sqrt{nk\log n}. \label{bdd:UCB}
\end{align} 
The regret in Theorem \ref{thm:UCB} differs only by a log factor compared to the optimal bound by ETC. Yet it does not require knowledge on the suboptimality gaps. This gains UCB popularity in many practical situations. Still, there are a few things to be improved here. First of all, the confidence level in Theorem \ref{thm:UCB} depends on horizon. This can be removed by choosing $\delta$ in an adaptive manner, say, $\delta_t = (1+t\log^2 t)^{-1}$, and the resulting bound on regret remains unchanged (even better from simulation results). Secondly, when calculating the UCB, the Hoeffding inequality is used, which can be rather loose sometimes. For example, consider the Bernoulli bandits whose means are close to $0$ or $1$. In such situations, one could apply the Chernoff bound instead, which gives a confidence interval based on relative entropy. The corresponding regret will be improved by a taming factor (which usually depends on variance). Also, UCB works without the subgaussian assumption on rewards. One could replace the sample mean estimator by the median-of-means estimator in defining UCB. For any distributions whose second moment exists, the regret is similar to what we have in Theorem \ref{thm:UCB} but with a worse constant factor. 

We would like to mention that the $\log n$ factor in \eqref{bdd:UCB} can be removed by making the confidence level arm-dependent. This is achieved in an algorithm termed Minimax Optimal Strategy in the Stochastic case, or MOSS.  As the name suggests, this algorithm achieves asymptotic optimality under the minimax criteria. Precisely, for arm $i$ in round $t$,  MOSS selects $\delta_{t, i}=\min\{1, (kT_i(t-1)/n)^2\}$. One can show that regret in this case satisfies
\begin{align*}
R_n\leq 38\sqrt{kn}+\sum_{i\in [k]}\Delta_i. 
\end{align*}
This is of the same order as the best possible minimax lower bound for stochastic bandits. However, the hidden cost is that that the variance of the pseudo-regret increases significantly.

Another algorithm called $\e$-Greedy is also built upon the philosophy of being optimistic. Instead of estimating via upper confidence bounds in the case of UCB, it takes a non-deterministic policy that forces exploration on arms which look sub-optimal. The details are given below:
\begin{algorithm}
\DontPrintSemicolon
 \KwIn{$(\e_t)_{t\in [n]}$: exploration parameters.}
 \KwOut{ $\pi=(\pi_t)_{t\in [n]}$}
 \While{$t\leq k$}{\begin{align*}
 \pi_t = t
 \end{align*}}
 \While{$k<t\leq n$}{
\begin{align*}
\pi_t \sim\begin{cases}
 \displaystyle\arg\max_{i\in [k]}\left\{\frac{1}{T_i(t-1)}\sum_{s\in [s-1]}x_{si}\mathbb{I}\{\pi_t = i\}\right\}\ \ \ \text{with probability}\ 1-\e_t\\
 \text{Uniform}([k])\ \ \ \text{with probability}\  \e_t
 \end{cases}
\end{align*}}
\caption{The $\e$-Greedy Algorithm. } 
\label{alg:e-greedy}
\end{algorithm}

Note that as $t$ becomes large, the time spent on exploration should diminish in order to get a good regret bound. Indeed, if choosing $\e_t\geq \e$ as a constant for all $t\in [n]$, one would expect 
\begin{align*}
\lim_{n\rightarrow\infty}\frac{R_n}{n} = \frac{\e}{k}\sum_{i\in [k]}\Delta_i. 
\end{align*}
By carefully choosing $\e_t$ as a decreasing function of $t$, we can get some comparable results as before:
\begin{Th}\label{thm:e-greedy}
Let $\Delta=\min_{i: \Delta_i>0}\Delta_i$ and $\e_t = \min\{1, Ct^{-1}\Delta^{-2}k\}$, where $C$ is some sufficiently large constant. Then, the regret under the $\e$-Greedy satisfies
\begin{align*}
R_n\leq C'\sum_{i\in [k]}\left(\Delta_i+\frac{\Delta_i}{\Delta^2}\log\left\{e, \frac{n\Delta^2}{k}\right\}\right),
\end{align*}
where $C'$ is an absolute constant. 
\end{Th}

Theorem \ref{thm:e-greedy} gives a similar bound to the one in Theorem \ref{thm:ETC} and \ref{thm:UCB}. The choice for $\e$, which is horizon-independent, requires only information on the smallest suboptimality gap. However, the unspecified constant $C$ may make performance of the algorithm uncertain in practice.  

Before proceeding further to discuss the lower bounds on regret, we give some numerical simulations to verify the bounds derived so far. The bandit in the following experiment is set as $2$-armed Gaussian with horizon $n=1000$. The suboptimality gap $\Delta$ is chosen in $10$ different levels: $[1:10]*10^{-1}$.  Algorithms including ETC ($m=20, 50, 80$ as well as the close-to-optimal choice $\max\left\{1, 4\Delta_2^{-2}\log(n\Delta_2^2/4)\right\}$), UCB, UCB with horizon-free confidence levels, MOSS and $\e$-Greedy ($C=0.2$) are tested. For fixed $\Delta$, the regret $R_n$ for each policy is calculated by averaging over $1000$ independent samples. The results are in Figure \ref{fig:1}. 
\begin{figure}[ht]
\centering
\includegraphics[width = 0.55\textwidth,clip,trim = 1cm 0cm 0cm 0cm]{sb}
\caption{Numerical simulations of several stochastic learning policies for two-armed Gaussian bandits with varying suboptimality gap.}
  \label{fig:1}
\end{figure}



\subsection{Lower bounds on regret}

To see how good the regret bounds derived in Algorithm \ref{alg:ETC} and \ref{alg:UCB}, we introduce the \emph{minimax} lower bound to quantify the comparison. Let $\V$ be the environment class, or equivalently, the set of admissible distributions on rewards: $\otimes_{i\in [k]}\nu_i$. The worst-case regret for a policy $\pi$ is defined as 
\begin{align*}
R_n(\pi, \V):=\sup_{\otimes_{i\in [k]}\nu_i\in\V}R_n(\pi).
\end{align*}
The minimax regret $R_n^*$ is the best worst-case regret among all possible policies $\Pi$: 
\begin{align}
R_n^*:=\inf_{\pi\in\Pi}R_n(\pi, \V).\label{s:mm}
\end{align}
Obtaining lower bounds on \eqref{s:mm} helps us understand the limit in stochastic bandits learning. We introduce a natural way to approach this problem.  The general idea is to show that for any $\pi\in\Pi$, there exist two environments $\nu, \nu'\in\V$ such that $R_{n,\nu}(\pi)$ and $R_{n,\nu'}(\pi)$ cannot be both small at the same time. Intuitively, a good instance for $\nu$ is bad for $\nu'$, and vice versa. If $\V$ is a parametric family, this implies that the parameter of $\nu$ is distant from the parameter of $\nu'$. On the other hand, $\pi$ should not distinguish $\nu$ from $\nu'$ so that $\E_\nu\approx\E_{\nu'}$, which requires $\nu$ to be close to $\nu'$. The right construction of $\nu$ and $\nu'$ is therefore relying on finding a trade-off point.  
\begin{Th}\label{s:minimax}
Let $\V$ be the class of normalized Gaussian bandits with mean $\mu=(\mu_i)_{i\in [k]}\in [0,1]^k$. Suppose that $n\geq k-1$. Then for any policy $\pi$ there exists some $\nu\in\V$ such that
\begin{align}
R_{n,\nu}(\pi)\geq\frac{1}{27}\sqrt{(k-1)n}. \label{s:6}
\end{align}  
\end{Th}
\begin{proof}
Fix a policy $\pi$. Let $\Delta\in [0, 1/2]$ be some parameter to be tuned later. Consider two bandits defined as
\begin{align*}
\nu &= (\Delta, 0, \cdots, 0)\\
\nu' &= (\Delta, 0, \cdots, \underbrace{2\Delta}_{\text{$i$-th component}}, 0, \cdots, 0),
\end{align*}
where $i$ is the least-played arm by $\nu$ on average: $i=\arg\min_{j>1}\E_\nu[T_j(n)]$. It is clear that spending too much time on arm $1$ is good for $\nu$ but bad for $\nu'$. Precisely, using the Chebyshev's inequality,
\begin{align}
R_{n, \nu}(\pi)+R_{n, \nu'}(\pi)\geq \frac{n\Delta}{2}\left(\P_\nu\left(T_1(n)\leq \frac{n}{2}\right)+\P_{\nu'}\left(T_1(n)> \frac{n}{2}\right)\right).\label{s:3}
\end{align}
The right-hand side of \eqref{s:3} can be bounded via Le Cam's method in statistics. A convenient form is to use the Bretagnolle-Huber inequality, which states that for probability measure $\P$ and $\Q$ on the same probability space $(\Omega, \mathcal{F})$, and $A\in\mathcal{F}$, 
\begin{align}
\P(A)+\Q(A^c)\geq\frac{1}{2}\max\left\{e^{-\textsf D_{KL}(\P||\Q)}, e^{-\textsf D_{KL}(\Q|| \P)}\right\},\label{s:4}
\end{align}
where $\textsf D_{KL}(\cdot || \cdot)$ is the KL-divergence. Let $A = \{T_1(n)\leq n/2\}$, and $\P, \Q$ be the $\pi$-induced measure on $([k]\oplus\R)^n$ under $\nu$ and $\nu'$, respectively. It can be checked by definition that 
\begin{align}
\textsf D_{KL}(\P, \Q)& = \sum_{j\in [k]}\E_\nu[T_j(n)]\textsf D_{KL}(\mu_j, \mu'_j)\label{s:5}\\
& = \E_\nu[T_i(n)]\textsf D_{KL}(\mu_i, \mu'_i)\nonumber\\
& \leq\frac{2n}{k-1}\Delta^2\nonumber,
\end{align}
where the last inequality follows from the fact that $\sum_{j\in [k]}\E_\nu[T_j(n)]=n$. Plugging \eqref{s:4} and \eqref{s:5} into \eqref{s:3} yields
\begin{align*}
R_{n, \nu}(\pi)+R_{n, \nu'}(\pi)\geq\frac{n\Delta}{4}e^{-\frac{2n\Delta^2}{k-1}}. 
\end{align*}
Taking $\Delta = \sqrt{\frac{k-1}{4n}}$ gives the desired result. 
\end{proof}
The constant $1/27$ in the bound \eqref{s:6} can be improved to $1/8$ via a different approach, which considers a sequence of environment instead of two, that are mutually inconsistent. The rest of the proof goes similarly by using the Pinsker's inequality.   

Theorem \ref{s:minimax} may be conservative if one is only interested in lower bounds on a set of `good' policies. In this case, one may derive a stronger result on the asymptotic behaviour of the regret appealing to \emph{instance-dependent} analysis, which is presented as follows.  

First of all, let us clarify what the good policies are. A policy $\pi$ is \emph{consistent} over $\V$ if for all $\nu\in\V$ and $p>0$, 
\begin{align*}
\lim_{n\rightarrow\infty}\frac{R_{n, \nu}(\pi)}{n^p} = 0. 
\end{align*}
Roughly speaking, the regret of a consistent policy is of logarithmic order asymptotically. Denote by $\Pi_{c}(\V)$ the set of consistent policies over $\V$. The following theorem implies that such intuition is accurate in a strong sense. 

\begin{Th}\label{s:ins}
Let $\nu\in\V = \otimes_{i\in [k]} \V_i$ and $1$ be the optimal arm. For any $\pi\in\Pi_c(\V)$,
\begin{align}
\liminf_{n\rightarrow\infty}\frac{R_{n,\nu}(\pi)}{\log n}\geq \sum_{i: \Delta_i>0}\frac{\Delta_i}{\mathsf d_{\inf} (\mu_i, \mu_1, \V_i)}, 
\end{align}
where $\mathsf d_{\inf} (\mu_i, \mu_1, \V_i) = \inf_{\nu_i'\in\V_i, \mu_i'>\mu_1}\textsf D_{KL}(\nu_i, \nu_i')$. 
\end{Th}
\begin{proof}
The idea of the proof is similar to Theorem \ref{s:minimax}. Thanks to \eqref{s:2} it suffices to prove that for $i\in [k]$ with nonzero suboptimality gap, 
\begin{align*}
\E_\nu[T_i(n)]\geq \mathsf d^{-1}_{\inf} (\mu_i, \mu_1, \V_i).
\end{align*}
To show this, consider a modified bandit $\nu'$ that agrees with $\nu$ except at the $i$-th arm, which equals $\nu_i'\in\V_i$ such that $\textsf D_{KL}(\nu_i, \nu_i') < \mathsf d_{\inf} (\mu_i, \mu_1, \V_i)+\e$ for some $\e>0$. Applying the Bretagnolle-Huber inequality with $A = \{T_i(n)\leq n/2\}$ and $\P, \Q$ the $\pi$-induced measure on $([k]\oplus\R)^n$ under $\nu$ and $\nu'$, respectively, 
\begin{align}
R_{n, \nu}(\pi)+R_{n, \nu'}(\pi)\geq\frac{n}{2}\min\{\Delta_i, \mu_i'-\mu_1\}e^{-(\mathsf d_{\inf} (\mu_i, \mu_1, \V_i)+\e)\E_\nu[T_i(n)]}.\label{s:7}
\end{align}
The left-hand side of \eqref{s:7} can be further bounded from above since $\pi$ is consistent. Indeed, for any $p>0$, there exists an absolute constant $c_p$ such that $R_{n, \nu}(\pi)+R_{n, \nu'}(\pi)<c_pn^p$. Plugging this in \eqref{s:7} and taking $n$, $\e$ and $p$ to $\infty$, $0$ and $0$ sequentially produces the desired result. 
\end{proof}
Theorem \ref{s:ins} is more of theoretical interest, as the asymptotic result often hides the worrying constants that matter in practice. However, from this point of view, it can be checked that MOSS is asymptotically optimal in the $1$-subgaussian environment.   


\section{Stochastic linear bandits}

\subsection{Stochastic contextual bandits and stochastic linear bandits}

Contextual structures can be made of good use in bandit learning. To incorporate such side-information, we further assume that players have access to the present context before selecting an arm in each round. Precisely, let $\mathcal C$ be the set of contexts and $\pi=(\pi_t)_{t\in [n]}\in [k]^n$ be a policy. $\pi_t$ is measurable with respect to the past decisions/rewards as well as the contexts revealed up to time $t$. The rewards collected by the player are 
\begin{align}
x_{t} = r(c_t, \pi_t)+\eta_{t}\ \ \ \ \ \ t\in [n],\label{sl:1}
\end{align}
where $c_t\in\mathcal C$ (deterministic or random) is the context in round $t$, $r(\cdot, \cdot):\mathcal C\times [k]\to\R$ is the reward function, and $\eta_{t}$ is the noise satisfying 
\begin{align}
\E[e^{\lambda\eta_{t}}|c_{1}, \pi_{1}, x_{1}, \cdots, c_{t}, \pi_{t}]\leq e^{\lambda^2/2}. \label{sl:2}
\end{align}
It is easy to check from \eqref{sl:2} that $\eta_t$ is subgaussian conditional on the history. \eqref{sl:1} and \eqref{sl:2} together lead to the so-called \emph{stochastic contextual bandits}. Due to the lack of knowledge of $r$, the regret is characterized by 
\begin{align}
R_n = \underbrace{\E\left[\sum_{c\in\mathcal C}\sum_{t: \pi_t = c}\max_{i\in [k]}r(c, i)\right]}_{(*)} - \E\left[\sum_{t\in [n]}x_{t}\right]. \label{sl:reg}
\end{align}
$(*)$ implicitly agrees that the best policy in each round is to play the arm with the largest expected reward in the present context, which is most meaningful if the dependence between contexts and actions is mild. A counterexample is when an optimal action at the beginning leads to unfavorable contexts in the subsequent rounds. The ideal $c_t$ under our consideration is either deterministic or independent of the past actions.  Conditional on $(c_t)_{t\in [n]}$, $R_n$ becomes a sum of regrets of $|\mathcal C|$ stochastic bandits. Heuristically, the best strategy is to treat the contexts separately and apply the optimal policy for each individual,  
\begin{align}
R_n=\E\left[\sum_{c\in\mathcal C}\E\left[R_{n,c}|c_1, \cdots, c_n\right]\right]\stackrel{\text{Thm.}\ \ref{s:ins}}{\lesssim}\E\left[\sum_{c\in\mathcal C}\sqrt{k\sum_{t\in [n]}\mathbb I_{c_t = c}}\right]\leq\sqrt{nk|\mathcal C|}.\label{sl:3}
\end{align} 
This immediately generalizes the results in Section \ref{sec1} to the stochastic contextual bandits, assuming nothing on the reward function $r$. However, in some practical situations, $|\mathcal C|$ can be astronomical (i.e., $|\mathcal C|>>\sqrt{n}$), making \eqref{sl:3} a vacuous bound.  The problem here is that our strategy only uses the combinatorial nature of $\mathcal C$. For example, supposing that $\mathcal C$ is a subset of a normed space and $r$ is continuous, one would expect $r(c, \cdot)$ and $r(c', \cdot)$ to be close if $||c-c'||$ is small. Yet this is not utilized in the above algorithm design which deals with elements in $\mathcal C$ in parallel. To utilize other information of $\mathcal C$, we could further hypothesize on the structures of $r$ (i.e., $r$ is in some parametric family). A simple choice gives rise to a class of interesting models which we will study next. 

Suppose that $r(c,i)$ is a linear function of a high-dimensional feature determined by $(c, i)$ via a kernel map $\phi(c,i): \mathcal C\times [k]\to\R^d$. That is, $r(c, i) = \langle\t, \phi(c,i) \rangle$ for some $\t\in\R^d$. In this case \eqref{sl:reg} can be rewritten as 
\begin{align}
R_n = \E\left[\sum_{t\in [n]}\max_{a\in\mathcal A_t}\langle\theta, a\rangle-x_{t}\right],\label{sl:4}
\end{align}  
where $\mathcal A_t = \{\phi(c_t, i)\}_{i\in [k]}$ is the set of features available at time $t$. Here $\theta$ is unknown, and $\mathcal A_t$ is a finite set which can be either deterministic or random. This particular parametrization of $r$ is called \emph{stochastic linear bandits}. 

We next discuss a few policies in stochastic linear bandits, depending on different assumptions on $\mathcal A_t$. Note that $\mathcal A_t$ is finite from the perspective of stochastic contextual bandits and the kernel map. However, in the general set-up of stochastic linear bandits, $\mathcal A_t$ is allowed to be infinite. Regardless of varying assumptions on $\mathcal A_t$, the regret defined in \eqref{sl:4} is based on the lack of information of $r$ so that one always needs to efficiently estimate for $\theta$ in the exploration process. Nevertheless, the efficiency here can have different meanings for different types of $\mathcal A_t$, which needs to be discussed in a case-dependent manner. 

\subsection{Stochastic linear bandits with finite and fixed action sets}

We begin our analysis for stochastic linear bandits whose action set $\mathcal A_t = \mathcal A$ does not change with time. We also assume that $|\mathcal A|=k$. Let $a_t\in\mathcal A$ be the feature vector associated with the action under some policy $\pi$ at time $t$. The exploration process suffices to estimate $\t$, and a natural estimator is the least-squares estimator. One may use the Ridge estimator if having invertibility issues. Precisely, in round $t$, $\t$ is estimated by $\hat{\t}_t$, where $\hat{\t}_t $ solves the normal equations: 
\begin{align*}
\underbrace{\begin{pmatrix}
a_1, \cdots, a_{t-1}
\end{pmatrix}}_{A^T_{t-1}}\underbrace{\begin{pmatrix}
a_1^T \\
\vdots \\
a_{t-1}^T
\end{pmatrix}}_{A_{t-1}}\t = \underbrace{\begin{pmatrix}
a_1, \cdots, a_{t-1}
\end{pmatrix}}_{A^T_{t-1}}\begin{pmatrix}
x_1 \\
\vdots \\
x_{t-1}
\end{pmatrix}.
\end{align*}
Note that the design matrix $A_{t-1}$ depends on the past actions. Since $\mathcal A$ is fixed, a good policy in the exploration stage should efficiently estimate the marginals of $\t$ along the directions in $\mathcal A$. In other words, for any $\e, \delta>0$, we wish to find a $\pi$ such that 
\begin{align}
T_{\pi}(\e, \delta):=\inf\left\{t: \max_{a\in\mathcal A}\underbrace{\P\left(|\langle\hat{\t}_t-\t, a\rangle|>\e\right)\leq\delta}_{\textcolor{blue}{\text{used to build c.i. for}\ \langle\t, a\rangle}}\right\}\label{sl:5}
\end{align}
is as small as possible. This is equivalent to the optimal experimental design problem in statistics. 
To see this, note that for fixed $a\in\mathcal A$, 
\begin{align*}
\P\left(|\langle\hat{\t}_t-\t, a\rangle|>\sqrt{2||a||^2_{V_{t-1}^{-1}}\log\left(\frac{2}{\delta}\right)}\right)\leq\delta,
\end{align*}
where $V_{t-1} = A^T_{t-1}A_{t-1}$. Setting 
\begin{align*}
\sqrt{2||a||^2_{V_{t-1}^{-1}}\log\left(\frac{2}{\delta}\right)}\leq\e
\end{align*}
and solving for $t$ yields
\begin{align*}
||a||_{V_{t-1}^{-1}}\leq\sqrt{\frac{\e^2}{2}\log^{-1}\left(\frac{2}{\delta}\right)}.
\end{align*}
Since $V_t^{-1}\preceq V^{-1}_{t-1}$, an upper bound for \eqref{sl:5}, which we denote by the same notation, is
\begin{align}
T_{\pi}(\e, \delta) = \inf\left\{t: \max_{a\in\mathcal A}||a||_{V_{t-1}^{-1}}\leq \sqrt{\frac{\e^2}{2}\log^{-1}\left(\frac{2}{\delta}\right)}\right\}.\label{sl:6}
\end{align}
Let $\mathcal P(\mathcal A)$ be set of probability measures on $\mathcal A$. There is a one-to-one correspondence between policies and $\mathcal P(\mathcal A)$ asymptotically (The frequency of choosing an action $a$ in $t$ rounds under $\pi$ can be roughly viewed as the probability $p(a)$). Denote by $p$ the probability measure associated with $\pi$. It is easy to verify that 
\begin{align*}
T_{\pi}(\e, \delta) = \frac{\max_{a\in\mathcal A}||a||^2_{A^{-1}(p)}}{\frac{\e^2}{2}\log^{-1}\left(\frac{2}{\delta}\right)}+1\ \ \ \ \ A(p):=\E_p[aa^T]
\end{align*}
Hence, as $t\to\infty$, minimizing \eqref{sl:6} over the possible policies is equivalent to the following optimization problem:
\begin{align}
\min_{p\in\mathcal P(\mathcal A)}\max_{a\in\mathcal A}||a||^2_{A^{-1}(p)}.\label{sl:7}
\end{align}
The term $\max_{a\in\mathcal A}||a||^2_{A^{-1}(p)}$ is often denoted by $g(p)$, and \eqref{sl:7} is referred to as the \emph{G-optimal design problem}. Solving \eqref{sl:7} relies on the following theorem due to Kiefer and Wolfowitz:
\begin{Th}[Kiefer-Wolfowitz, 1960]\label{KW}
Let $\mathcal A\subset\R^d$ be a compact set and $\text{span}(\mathcal A)=\R^d$. Then
\begin{align*}
p^*\text{solves \eqref{sl:7}}\Longleftrightarrow p^*\in\argmax_{p\in\mathcal P(\mathcal A)}\log\det A(p)\Longleftrightarrow g(p^*) = d. 
\end{align*}
Moreover, there exists a $p^*$ such that $\text{supp}(p^*)\leq d(d+1)/2$. 
\end{Th}
\begin{Rem}
The geometric interpretation of the set $\left\{||x||_{A^{-1}(p^*)}\leq d\right\}$ based on duality is the minimum volume enclosing ellipsoid for the set $\mathcal A$. When $\mathcal A$ is convex, this is known as \emph{John's ellipsoid}.  
\end{Rem}
\begin{Rem}
According to Theorem \ref{KW}, solving the $G$-optimal design problem is the same as finding the maximizer for $\log\det A(p)$, which is a convex optimization problem and can be approximately solved via Frank-Wolfe algorithm: For $a\in\mathcal A$ and $a_k = \argmax_{a\in\mathcal A}||a||_{A^{-1}(p_k)}^2$, 
\begin{align*}
p_{k+1}(a) = \left(1-\frac{\frac{1}{d}||a_k||_{A^{-1}(p_k)}-1}{||a_k||_{A^{-1}(p_k)}-1}\right)p_k(a) + \frac{\frac{1}{d}||a_k||_{A^{-1}(p_k)}-1}{||a_k||_{A^{-1}(p_k)}-1}\mathbb I_{a=a_k}.
\end{align*}
If $p_0$ is a uniform distribution on $\mathcal A$, then $g(p_k)\leq (1+\e)d$ for $k=\mathcal O(d\log\log |\mathcal A|+d/\e)$.
\end{Rem}

Once \eqref{sl:7} is (approximately) solved, one can use the solution to construct a near-optimal policy for exploration. Algorithms in Section \ref{sec1} like ETC naturally extend to this case. However, similar problems still exist as the best trade-off point usually depends on the suboptimality gaps. An alternative approach is to take the exploration adaptively together with a phase-elimination process, which is given below:

\begin{algorithm}
\DontPrintSemicolon
 \KwIn{Action set $\mathcal A$ and $\delta$.}
 \KwOut{ $\{a_t\}_{t\in [n]}.$}
 \textbf{Initialization}: $\ell = 0$, $\mathcal A_1 = \mathcal A$, $T_0 = 0$, $d_0=d$ and $N_0 = 0$.\\
 \While{$N_\ell\leq n$}{
 \begin{itemize}
 \item $\ell = \ell + 1$;
 \item $\e_\ell = 2^{-\ell}$;
 \item $p_\ell \in\argmax_{p\in\mathcal P(\mathcal A_\ell)}\log\det \mathcal A_\ell(p)$ and $\text{supp}(p_\ell)\leq d_{\ell-1}(d_{\ell-1}+1)/2$;
 \item $d_\ell = \min\{|\mathcal A_\ell|, d\}$.
 \end{itemize}
 \For{$a\in\mathcal A_\ell$}{
 $T_\ell(a)=\displaystyle\left\lceil 2\e_\ell^{-2}d_\ell p_\ell(a)\log\left(\frac{2k^2\ell(\ell+1)}{\delta}\right)\right\rceil$.}
\begin{itemize}
\item $T_\ell = \sum_{a\in\mathcal A}T_\ell(a)$;
\item For $t\in [T_{\ell-1}+1,T_\ell]$ choose action $a\in\mathcal A_\ell$ $T_\ell(a)$ times. Denote the  design matrix by $A_\ell$. 
\item $\hat{\t}_\ell = (A^T_\ell A_\ell)^{-1}A_\ell^T(x_{T_{\ell-1}+1}\cdots, x_{T_\ell})^T$;
\item $\mathcal A_{\ell+1} = \{a\in\mathcal A_\ell, \max_{b\in\mathcal A_\ell}\langle\hat{\t}_\ell, b-a\rangle\leq 2\e_\ell\}$; 
\item $N_\ell = N_{\ell-1} + T_\ell$. 
\end{itemize}
}
\caption{Phase-elimination with $G$-optimal exploration algorithm. } 
\label{alg:sl-finite}
\end{algorithm}

Assume additionally that the largest suboptimality gap is bounded by $1$. Then the regret analysis of Algorithm \ref{alg:sl-finite} can be summarized in the following theorem:

\begin{Th}\label{sl:finite}
The psudo-regret $\bar{R}_n=n\max_{a\in\mathcal A}\langle\t, a\rangle-\sum_{t\in [n]}\langle\t, a_t\rangle$ in Algorithm \ref{alg:sl-finite} satisfies
\begin{align*}
\P\left(\bar{R}_n> 32\left(\sqrt{nd\log\left(\frac{k\log n}{\delta}\right)}+d^2\right)\right)\leq \delta, 
\end{align*}
\end{Th}
Moreover, integrating the tail probability,
\begin{align*}
R_n\leq 64(\sqrt{nd\log (k\log n)}+d^2). 
\end{align*}
\begin{proof}
Algorithm \ref{alg:sl-finite} guarantees a high probability that (i). an optimal action $a^*$ is always in $\mathcal A_\ell$ , and (ii). suboptimal arms can only survive finitely many times. In fact, 
\begin{align*}
&\P\left(a^*\in\mathcal A_\ell\  \text{for only finitely many}\ \ell \right)\\
\leq&\ \  \sum_{\ell\in\N}\P\left(a^*\notin\mathcal A_{\ell+1}|a^*\in\mathcal A_{\ell}\right)\\
\leq&\ \ \sum_{\ell\in\N}\sum_{\substack{a\in\mathcal A_\ell\\ a\neq a^*}}\left(\P\left(|\langle\hat{\t}_{\ell+1}-\t, a^*\rangle|>\e_{\ell+1}\right)+\P\left(|\langle\hat{\t}_{\ell+1}-\t, a\rangle|>\e_{\ell+1}\right)\right)\leq\frac{\delta}{k}.
\end{align*}
On the other hand, let $\Delta_a = \langle\t, a^*-a\rangle$ be the suboptimality gap between $a$ and $a^*$ and define $\ell_a = 2+\lceil\log\Delta_\ell^{-1}\rceil$ (base 2). Then, 
\begin{align*}
\P\left(a\in\mathcal A_{\ell_a}\right)&\leq \P\left(|\langle\hat{\t}_{\ell_a}-\t, a^*\rangle|>\e_{\ell_a}\right)+\P\left(|\langle\hat{\t}_{\ell_a}-\t, a\rangle|>\e_{\ell_a}\right)\leq \frac{\delta}{k}. 
\end{align*}
Taking a union bound gives
\begin{align}
\P\left(a^*\in\bigcap_{\ell\in\N}\mathcal A_\ell\ \&\ a\notin \mathcal A_{\ell_a}\ \text{for all $a\in\mathcal A$ with $\Delta_a>0$} \right)\geq 1-\delta. \label{sl:8}
\end{align}
We now estimate $\bar{R}_n$ conditional on the event defined in \eqref{sl:8}. Let $\Delta>0$ be some parameter to be tuned later. We first consider the actions played in the first $n$ rounds with suboptimality gap less than $\Delta$. It is clear that the cumulative loss incurred by these actions is bounded by $n\Delta$. On the other hand, the actions with suboptimality gap greater than $\Delta$ can be played only in the first $\ell_\Delta$ rounds, where $\ell_\Delta = 1+\lceil\log\Delta^{-1}\rceil\leq 2+\log\Delta^{-1}$. The regret of these actions is bounded by the total loss incurred in first $\ell_\Delta$ rounds of exploration, which is further bounded by  
\begin{align*}
\sum_{\ell = 1}^{\log n\wedge \ell_\Delta}4\e_\ell T_\ell &\leq \frac{64}{\Delta}d\log\left(\frac{4k^2\log^2 n}{\delta}\right) + 4d^2,
\end{align*} 
where for $\ell=1$ we used that the maximum of suboptimality gaps is less than $1\leq 4\e_1$. Therefore, 
\begin{align*}
\bar{R}_n&\leq\inf_{0<\Delta<1}\left\{n\Delta+\frac{64}{\Delta}d\log\left(\frac{4k^2\log^2 n}{\delta}\right)+4d^2\right\}\\
&\leq 16\sqrt{nd\log\left(\frac{4k^2\log^2 n}{\delta}\right)}+4d^2\\
&\leq 32\left(\sqrt{nd\log\left(\frac{k\log n}{\delta}\right)}+d^2\right). 
\end{align*}
The proof is thus complete. 
\end{proof}
\begin{Rem}
One advantage of Algorithm \ref{alg:sl-finite} is that it behaves well even when the model is misspecified. Particularly, if the observation $x_t$ is contaminated by some noise of magnitude less than $\e$, then it can be proved that 
\begin{align*}
R_n=\mathcal O\left(\sqrt{nd\log (kn)}+\e n\sqrt{d}\log n\right). 
\end{align*}
\end{Rem}

\begin{Rem}
Taking $\mathcal A$ as canonical bases in $\R^d$ and $d=k$,  Algorithm \ref{alg:sl-finite} reduces to an elimination algorithm for stochastic bandits. Theorem \ref{sl:finite} tells us that the asymptotic regret is $\mathcal O(\sqrt{nk\log\log n})$.   
\end{Rem}

\subsection{Stochastic linear bandits with general action sets}

We now consider stochastic linear bandits with general action sets, which means that $\mathcal A_t$ can be both time-varying and infinite.
This requires us to find all-rounded confidence sets adaptively.  
Suppose for now that $\t$ is still estimated by least-squares estimators. 
A possible choice for confidence sets is a sequence of balls weighted by the design matrices, similar to the ones used in the classical linear regression.
However, the analysis here is more intricate due to the adaptive nature of the design matrices. 
We will also provide a different approach to constructing the confidence sets based on online learning prediction, which allows one to utilize the prior knowledge of $\t$ in the construction. 
Except for the confidence sets, the general algorithm resembles the UCB algorithm in Section \ref{sec1}. 
This implies that $\t$ is estimated stepwise and the decision is made under optimistic bias.  
Therefore, it is thus necessary to use regularized least-squares estimators to avoid invertibility issues (at the beginning). Particularly, let $\lambda>0$ be a regularization parameter and our estimation for $\t$ at time $t$ is given by 
\begin{align*}
\hat{\t}_t = (\underbrace{\lambda I +A^T_{t-1}A_{t-1}}_{V_t(\lambda)})^{-1}A^T_{t-1}\begin{pmatrix}
x_1 \\
\vdots \\
x_{t-1}
\end{pmatrix}.
\end{align*}
Suppose that the confidence set at $t$ is an $V^{-1}_{t-1}(\lambda)$-weighted ball centered at $\hat{\t}_t$\footnote{Taking $\hat{\t}_t$ as the regularized least-squares estimator is convenient and gives heuristics on the shape of the confidences sets. Other reasonable estimators also work in the following analysis.}, i.e., $\mathcal C_t = \{z\in\R^d: ||z-\hat{\t}_t||^2_{V_{t-1}(\lambda)}\leq\beta_t\}$, where $\beta_t\geq 1$ is an increasing sequence computed from $\mathcal A_1, a_1, x_1, \cdots, A_{t-1}, a_{t-1}, x_{t-1}$. Suppose also that for fixed $n$ and $\delta$, the choice of $\beta_t$ makes $\mathcal \{\mathcal C_t\}_{t\in [n]}$ form a uniform \footnote{The reason why one needs uniform confidence sets is because the algorithm is built adaptively so that estimations at different steps are different.} $(1-\delta)$-confidence set:
\begin{align}
\P(\t\in\cap_{t=1}^n\mathcal C_t)\geq 1-\delta.\label{sl:9}
\end{align} 
Practicality of \eqref{sl:9} will be justified in a moment. A version of the UCB algorithm for stochastic linear bandits can then be described as follows:
\begin{algorithm}
\DontPrintSemicolon
 \KwIn{Action set $\mathcal A_t$, $\delta$ and $\lambda$.}
 \KwOut{ $\{a_t\}_{t\in [n]}.$}
 \textbf{Initialization}: $t=1$, $V_1(\lambda) = \lambda I$ and $a_1\in\mathcal A_1$.\\
 \While{$t\leq n$}{
 \begin{itemize}
\item $\hat{\t}_{t+1} = V^{-1}_{t}(\lambda)A_{t}^T(x_1, \cdots, x_{t})^T$;
\item $\beta_{t+1}=\beta_{t+1}(\mathcal A_1, a_1, x_1, \cdots, A_{t}, a_{t}, x_{t})$;
\item $\mathcal C_{t+1} = \{z\in\R^d: ||z-\hat{\t}_{t+1}||^2_{V_{t}(\lambda)}\leq\beta_{t+1}\}$;
\item $a_{t+1} \in \argmax_{a\in\mathcal A_{t+1}}\sup_{z\in\mathcal C_{t+1}}\langle z, a\rangle$;
\item $V_{t+1}(\lambda) = V_{t}(\lambda)+a_{t+1}a_{t+1}^T$; 
\item $t = t +1$. 
\end{itemize}
}
\caption{UCB algorithm for stochastic linear bandits (LinUCB).} 
\label{alg:sl-UCB}
\end{algorithm}

Algorithm \ref{alg:sl-UCB} is also known as Linear Reinforcement Learning (LinRel). Before discussing in detail how $\beta_t$'s are chosen, we give a general result on the asymptotic regret that Algorithm \ref{alg:sl-UCB} can achieve, as summarized in the following theorem:

\begin{Th}
Assume that for some $L>0$, 
\begin{align}
\sup_{a\in\cup_{t\in\N}\mathcal A_t}\left\{|\langle\t, a\rangle|, \frac{||a||_2}{L}\right\}\leq 1. \label{sl:10}
\end{align}
\end{Th}
For convenience we also assume that $\mathcal A_t$ is closed. Then, with probability at least $1-\delta$, the pseudo-regret $\bar{R}_n=\sum_{t\in [n]}\sup_{a\in\mathcal A_t}\langle\t, a-a_t\rangle$ in Algorithm \ref{alg:sl-UCB} satisfies that
\begin{align}
\bar{R}_n\leq 2+\sqrt{8n\beta_n\log\left(\frac{\det V_n(\lambda)}{\det V_1(\lambda)}\right)}\leq 2+\sqrt{8dn\beta_n\log\left(\frac{d\lambda+nL^2}{d\lambda}\right)}. \label{sl:11}
\end{align}
\begin{proof}
It suffices to check that \eqref{sl:11} holds on the event defined in \eqref{sl:9}. The assumptions in the theorem imply that $\mathcal A_t$ is compact thus there exists $z_t\in\mathcal C_t$ such that $\langle z_t, a_t\rangle = \sup_{z\in\mathcal C_t}\langle z, a_t\rangle$. Then,
\begin{align*}
\bar{R}_n = \sum_{t\in [n]}\sup_{a\in\mathcal A_t}\langle\t, a-a_t\rangle&\stackrel{\eqref{sl:10}}{\leq} 2+ \sum_{t=2}^n\min\left\{\underbrace{\left(\sup_{a\in\mathcal A_t}\langle\t, a\rangle-\langle z_t, a_t\rangle\right)}_{\leq 0 \ \text{conditional on}\ \eqref{sl:9}}+\langle z_t-\t, a_t\rangle, 2\right\}\\
&\stackrel{\eqref{sl:9}}{\leq} 2+2\sum_{t=2}^n \min\left\{\sqrt{\beta_n}||a_t||_{V^{-1}_{t-1}(\lambda)}, 1\right\}\\
&\stackrel{\beta_n\geq 1}{\leq} 2+2\sqrt{n\beta_n\sum_{t=2}^n\min\left\{||a_t||^2_{V^{-1}_{t-1}(\lambda)}, 1\right\}}\\
&\leq 2+2\sqrt{2n\beta_n\sum_{t=2}^n\log (1+||a_t||^2_{V^{-1}_{t-1}})},
\end{align*} 
where the last step uses the inequality $\min\{1, u\}\leq 2\log(1+u)$ for $u\geq 0$. Since $V_t$ is obtained from $V_{t-1}$ through a rank-1 update, it is easy to check that $\prod_{t=2}^n (1+||a_t||^2_{V^{-1}_{t-1}})=\frac{\det V_n(\lambda)}{\det V_1(\lambda)}$. Plugging this into the above estimate completes the proof of the first part in \eqref{sl:11}. The second part follows immediately by noting that $\det V_n(\lambda)\leq (\frac{1}{d}\text{tr}(V_n(\lambda)))^d\leq (\frac{d\lambda + nL^2}{d})^d$.
\end{proof}
\begin{Rem}
The constant $2$ comes from exploring the first action arbitrarily. It does not affect the asymptotic result and will be omitted later for convenience. 
\end{Rem}
\begin{Rem}
It can be seen from the above proof that how $\t$ is estimated at each step does not really matter, as long as the confidence set is weighted by $V_{t-1}$ and centered at the estimation. One may use other reasonable estimators in both theoretical analysis and practice, see section \ref{sec:olp}. 
\end{Rem}

We next give two ways of finding valid $\beta_t$'s to achieve \eqref{sl:9}, both of which used the maximal inequality for self-normalized martingales. 

\subsubsection{Laplace's method}

Define $S_t = A_{t}^T(\eta_1, \cdots, \eta_t)^T$. Note that for $t\in [n]$, 
\begin{align}
||\hat{\t}_t -\t||_{V_{t-1}(\lambda)}&=\langle V^{-1}_{t-1}(\lambda)(\lambda\t + S_{t-1}), \lambda\t + S_{t-1}\rangle^{1/2}\nonumber\\
& = ||\lambda\t + S_{t-1} ||_{V^{-1}_{t-1}(\lambda)}\leq  \lambda ||\t||_{V^{-1}_{t-1}(\lambda)} + ||S_{t-1} ||_{V^{-1}_{t-1}(\lambda)}\nonumber\\
&\stackrel{V^{-1}_{t-1}(\lambda)\preccurlyeq\lambda^{-1}I}{\leq} \sqrt{\lambda} ||\t||_2 + ||S_{t-1} ||_{V^{-1}_{t-1}(\lambda)}\label{sl:12}, 
\end{align}
Therefore it suffices to obtain a high-probability bound for $||S_{t-1} ||_{V^{-1}_{t-1}(\lambda)}$. The weight factor  is given by the inverse of $V_{t-1}(\lambda)$ which can be difficult to analyze. To get around we rewrite $\frac{1}{2}||S_t||^2_{V^{-1}_{t}(\lambda)}$ using the Legendre transform:
\begin{align*}
\frac{1}{2}||S_t ||^2_{V^{-1}_{t}(\lambda)} = \max_{x\in\R^d}\left(\langle x, S_t\rangle - \frac{1}{2}\langle x, x\rangle_{V_t(\lambda)}\right).
\end{align*}
Thanks to the subgaussian assumption on the noise, for fixed $x$, $M_t(x)=e^{\langle x, S_t\rangle - \frac{1}{2}\langle x, x\rangle_{V_t(\lambda)}}$ is a non-negative supermartingale started at $e^{-\lambda ||x||_2^2/2}$. This raises hope for a uniform bound in $t$ (as desired in \eqref{sl:9}) by appealing to the maximal inequality. However, there is an extra maximum over $x$ which needs to be addressed first. For this we use a technique called self-normalization. Self-normalization is similar to the Laplace's method in asymptotic analysis.  
Note that $||\lambda\t + S_{t-1} ||_{V^{-1}_{t-1}(\lambda)}$ grows to infinity as $t\to\infty$ (otherwise there is no need to do the analysis here!).  For large $t$, if one wants to understand $\max_{x\in\R^d}\log M_t(x)$, one may study $\bar{M}(x):=\log\int_{x\in\R^d}M_t(x)p(x)dx$ instead, where $p(x)$ is some finite-measure whose support contains $\argmax_{x\in\R^d} M_t(x)$. 
On the other hand, if $p(x)$ is a probability measure, averaging over $x$ preserves the martingale property so that $\bar{M}(t)$ is also a non-negative supermatingale. Take $p(x)$ as a gaussian measure on $\R^d$ with covariance matrix $\lambda^{-1} I$. Then,
\begin{align*}
\bar{M}_t = \left(\frac{\lambda^d}{\det (\lambda I+V_t(\lambda))}\right)^{1/2}e^{\frac{1}{2}||S_t||^2_{(\lambda I+V_t(\lambda))^{-1}}}\geq  \left(\frac{\lambda^d}{\det (2V_t(\lambda))}\right)^{1/2}e^{\frac{1}{4}||S_t||^2_{V^{-1}_t(\lambda)}}.
\end{align*}
By Doob's maximal inequality for non-negative supermartingales, 
\begin{align*}
&\P\left(\sup_{t\in\N}||S_t||_{V^{-1}_t(\lambda)}\geq 2\sqrt{2\log\left(\frac{1}{\delta}\right)+\log\left(\frac{\det V_t(\lambda)}{\lambda^d}\right)}\right)\\
\leq&\ \P\left(\sup_{t\in\N}\log\bar{M}_t\geq\log\left(\frac{1}{2^{d/2}\delta}\right)\right) = \P\left(\sup_{t\in\N}\bar{M}_t\geq \frac{1}{2^{d/2}\delta}\right)\leq 2^{d/2}\delta\E[\bar{M}_0]=\delta. 
\end{align*}
This together with \eqref{sl:12} implies that $\beta_t$ in \eqref{sl:9} can be chosen as 
\begin{align}
\sqrt{\beta_t} &= \sqrt{\lambda}||\t||_2+2\sqrt{2\log\left(\frac{1}{\delta}\right)+\log\left(\frac{\det V_t(\lambda)}{\lambda^d}\right)}\nonumber\\
&\leq\sqrt{\lambda}||\t||_2+2\sqrt{2\log\left(\frac{1}{\delta}\right)+d\log\left(\frac{d\lambda+nL^2}{d\lambda}\right)}\label{sl:13}.
\end{align}
Assuming $||\t||_2\leq\mathcal O(\sqrt{d})$, plugging \eqref{sl:13} into \eqref{sl:11} and integrates the tail probability yields that 
\begin{align*}
R_n = \mathcal O \left(d\sqrt{n}\log (nL)\right).
\end{align*}

\subsubsection{Online learning prediction}\label{sec:olp}

Algorithm \ref{alg:sl-UCB} (with $\hat{\t}_t$ being treated as a general estimator) is closely related to the problem of online learning prediction. 
In online learning prediction, given an action, the learner predicts the reward using some predictor built from the history before observing the reward. 
The true reward is then fed back to the learner to update the predictor. 
In the following we will use $\hat{x}_t$ as the predicted reward at time $t$, with its accuracy is measured by the cumulative squared-loss $\sum_{t\in [n]}(\hat{x}_t-x_t)^2$. 
In Algorithm \ref{alg:sl-UCB}, one still has the predictor beforehand. What makes it different is the goal of learning. One needs to decide him/herself which action to play and the way how this is done is by computing multiple predicted values using appropriate confidence sets. 

In the linear stochastic bandits, the best theoretic predictor is given by the expected reward function. For any predictor $\hat{x}_t$, it is meaningful to define the regret of $\hat{x}_t$ relative to a linear predictor $z\in\R^d$:
\begin{align*}
\rho_n(z) = \sum_{t\in [n]}(\hat{x}_t-x_t)^2-\sum_{t\in [n]}(\langle z, a_t\rangle-x_t)^2,
\end{align*}
where $a_t$ are given (either by a policy or some mysterious person). For any prior knowledge on $\t$ such that $\t\in\Theta\subset\R^d$, we say that the learner enjoys guarantee $B_n$\footnote{$B_n$ is a random quantity depending on data.} on $\Theta$ if $\sup_{z\in\Theta}\rho_n(z)\leq B_n$. As we will see, $B_n$ plays a crucial role in our construction of confidence sets, and allows some side-information on $\Theta$ to be incorporated.  

From now on we assume that a predictor $\hat{x}_t$ is given to estimate the reward. For example, taking $\hat{x}_t (\cdot)= \langle\hat{\t}_t, \cdot\rangle$ recovers the one in Algorithm \ref{alg:sl-UCB}. Since the confidence sets ask for an estimate for the mean reward, for any sequence of actions $\{a_t\}_{t\in [n]}$, we consider $Q_n := \sum_{t\in [n]}(\hat{x}_t-\langle\t, a_t\rangle)^2$. Using the Cosine theorem, it is easy to check that 
\begin{align*}
Q_n&=\rho(\t)+2\sum_{t\in [n]}\eta_t(\hat{x}_t-\langle\t, a_t\rangle)\leq B_n+2\underbrace{\sum_{t\in [n]}\eta_t(\hat{x}_t-\langle\t, a_t\rangle)}_{\texttt{m}_n}.
\end{align*}
To further bound $Q_n$ uniformly for $n\in\N$, observe that $e^{x\texttt{m}_t-\frac{x^2}{2}Q_t}$ is a non-negative supermartingale for every $x\in\R$. Using a similar self-normalization trick as in the previous analysis and setting $x = 1$ yields that 
\begin{align}
\P\left(\forall t\in\N \ \text{s.t.}\  |\texttt{m}_t|\leq\sqrt{(1+Q_t)\log\left(\frac{1+Q_t}{\delta^2}\right)} \right)\geq 1-\delta. \label{sl:14}
\end{align}
Conditional on the event defined in \eqref{sl:14}, for every $t\in\N$, 
\begin{align*}
Q_t\leq B_t + 2\sqrt{(1+Q_t)\log\left(\frac{1+Q_t}{\delta^2}\right)},
\end{align*}
which implies
\begin{align}
Q_t\leq 1+2B_t+32\log\left(\frac{\sqrt{8}+\sqrt{1+B_t}}{\delta}\right).\label{sl:15}
\end{align}
Although \eqref{sl:15} is a confidence set, it is possible to be unbounded and does not fit into the form used in Algorithm \ref{alg:sl-UCB}. To solve this, we restrict the confidence sets to a ball $B(L)$ of radius $L$ using \eqref{sl:10} , then identify an estimator for $\t$ based on the given predictor $\hat{x}_t$ and derive a confidence set around it. Let $\tilde{\t}_t$ be the regularized least-squares solution for the predicted values:
\begin{align*}
\tilde{\t}_t = V_t^{-1}(\lambda)A_t^T\underbrace{\begin{pmatrix}
\hat{x}_1\\
\vdots\\
\hat{x}_t
\end{pmatrix}}_{\hat{X}_t}.
\end{align*}
We can then rewrite $Q_t+\lambda ||\t||_2^2$ as 
\begin{align*}
&||A_t\t-A_t\tilde{\t}_t+A_t\tilde{\t}_t-\hat{X}_t||_2^2 + \lambda ||\t||_2^2\\
=&\  ||\t-\tilde\t_t||^2_{V_t(\lambda)} -\lambda ||\t-\tilde\t_t||_2^2 + ||A_t\tilde{\t}_t-\hat{X}_t||_2^2+2\langle\t-\tilde\t_t, \tilde\t_t\rangle_{V_t(\lambda)}-2\lambda \langle \t-\tilde \t_t, \tilde\t_t\rangle-2\langle\t-\tilde\t_t, \tilde\t_t\rangle_{V_t(\lambda)}+\lambda ||\t||_2^2\\
=&\ ||\t-\tilde\t_t||^2_{V_t(\lambda)}+ ||A_t\tilde{\t}_t-\hat{X}_t||_2^2+\lambda ||\tilde\t_t||_2^2.
\end{align*} 
It is then clear that 
\begin{align*}
&\left\{z\in\R^d\cap B(L): Q_t\leq 1+2B_t+32\log\left(\frac{\sqrt{8}+\sqrt{1+B_t}}{\delta}\right) \right\}\\
\subset&\ \left\{z\in\R^d: Q_t +\lambda ||z||_2^2\leq 1+2B_t+32\log\left(\frac{\sqrt{8}+\sqrt{1+B_t}}{\delta}\right) +\lambda L^2\right\}\\
\subset&\ \underbrace{\left\{z\in\R^d: ||\t-\tilde\t_t||^2_{V_t(\lambda)}\leq \underbrace{1+2B_t+32\log\left(\frac{\sqrt{8}+\sqrt{1+B_t}}{\delta}\right)+\lambda L^2}_{\beta_{t+1}} \right\}}_{\mathcal C_{t+1}}. 
\end{align*}
This together with \eqref{sl:11} implies that 
\begin{align*}
R_n = \mathcal O \left(\sqrt{nd(\E[B_n]+L^2)\log(nL)}\right)\stackrel{\text{if}\ L=\mathcal O(1)}{=}\left(\sqrt{nd\E[B_n]\log(nL)}\right).
\end{align*}
If $\t$ is known to be $s$-sparse, we can take $\Theta = \{z\in\R^d: ||z||_0\leq s\}$. In this case, it can be shown that $\E[\beta_n] \lesssim s\log n$. Hence we have $R_n = \mathcal O \left(\sqrt{nds\log^2(nL)}\right)$.
\begin{Rem}
When building up the confidence sets in the sparsity case, one needs to know in advance the sparsity level, which determines the choice for $B_t$. 
\end{Rem} 

\subsection{Asymptotic instance lower bounds}
We now discuss the inevitable regret for any learning strategy in stochastic linear bandits. As in Section \ref{sec1}, there are two directions to answer this question.  The first is to consider the minimax regret for fixed horizon $n$, which comes down to analyzing policies whose worst regret over a class of bandits is optimal. The idea for this approach is similar to the study of stochastic bandits. However, identifying the environments that are mutually incoherent for a given policy is more complicated due to the varying geometry of the action set. We shall not go into the details here. The take-away message is the conclusion: For various types of $\mathcal A$ (independent of $t$), the regret incurred by applying Algorithm \ref{alg:sl-UCB} is near-optimal (only differ by a log factor). From this perspective, optimistic algorithms are almost finite-time optimal. 

The other way is to consider the asymptotic lower bound for the regret of a consistent policy over a class of environments.  In this case, one first fixes a bandit then sends $n$ to infinity. This approach provides a theoretical limit of learning in infinite horizon, yet it may obscure some humongous constants that often matter in practice. The main result we are going to introduce in this section quantifies the asymptotic lower bound for consistent policies. The bound, which is solved by an optimization problem, is achievable by a well-designed policy (not guaranteed as finite-time optimal so may not be useful in practice).  As a disturbing consequence, it can be shown that optimistic policies can be far from being asymptotically optimal. We will mainly follow the paper \cite{lattimore2016end}. 

The following theorem shows that any suboptimal arm must be fully explored under any consistent policy. 

\begin{Th}\label{sl:asym}
Assume that $\mathcal A\subset\R^d$ is finite, independent of time and spans $\R^d$. Let $\t\in\R^d$ be the true parameter and $\{a_t\}_{t\in [n]}$ be a sequence of actions under a consistent policy over the complete environments $\R^d$. Suppose that there is a unique optimal action $a^*$ for $\t$. Define $\bar{G}_n = \E[\sum_{t\in [n]}a_ta_t^T]$. Then for any $a\in\mathcal A\setminus a^*$, 
\begin{align}
\limsup_{n\to\infty}\log n ||a||^2_{G_n^{-1}}\leq\frac{\Delta_a^2}{2}, \label{sl:16}
\end{align} 
where $\Delta_a = \langle\t, a^*-a\rangle$ is the suboptimality gap of $a$. 
\end{Th}
Note that the left-hand side of \eqref{sl:16} appears as the marginal error in the $(1-1/n)$-confidence interval of estimating $\langle\t, a\rangle$ by the least-squares estimator. In this sense we can conclude that $a$ is well-explored. 
\begin{proof}
We first note that $\bar{G}_n$ is invertible for sufficiently large $n$, otherwise some dimension has not been explored at all so that one can modify $\t$ to have linear regret. Let $\Delta = \min_{a\in\mathcal A\setminus a^*}\Delta_a$ and consider a new bandit defined by 
\begin{align*}
\t' = \t + \frac{\Delta_a + \e}{||a-a^*||^2_H}H(a-a^*),
\end{align*}
where $H$ is some positive semidefinite matrix with $||a-a^*||_H>0$, and $\e\in (0, \Delta]$. It is easy to check that action $a$ under $\t'$ is better than $\t$ by $\e$. Therefore, applying the Bretagnolle-Huber inequality on the event that $T_a(n):=\sum_{t\in [n]}\mathbb I_{a_t = a}\geq n/2$, with the measures being the induced measures under $\t$ and $\t'$, respectively, 
\begin{align*}
c_p n^p\stackrel{\text{consistency}}{\geq} R_{n, \t}+R_{n, \t'} &\stackrel{\Delta_a\geq\e}{\geq}\frac{n}{2}\e\left(\P_\t\left(T_a(n)>\frac{n}{2}\right)+\P_{\t'}\left(T_a(n)\leq\frac{n}{2}\right)\right)\\
&\ \ \geq \frac{n}{4}\e e^{-\frac{1}{2}\mathsf D_{KL}(\t, \t')} =  \frac{n}{4}\e e^{-\frac{1}{2}||\t-\t'||^2_{\bar{G}_n}}\\
&\ \ =\frac{n}{4}\e e^{-\frac{1}{2}\frac{(\Delta_a+\e)^2}{||a-a^*||_H^4}||a-a^*||^2_{H\bar{G}_nH}},
\end{align*}
for any $p>0$ and $c_p>0$ only depends on $p$. 
Rearranging terms, setting $p\to 0$ first then $\e\to 0$ yields that
\begin{align}
\liminf_{n\to\infty}\frac{\Delta_a^2 ||a-a^*||^2_{H\bar{G}_nH}}{2\log n||a-a^*||_H^4}\geq 1. \label{sl:17}
\end{align}
For simplicity assume that $\log n ||a-a^*||_{G_n^{-1}}^2$ converges to $c\in [0, \infty]$ (so that $\lim =\limsup$), otherwise consider any convergent subsequence.  Take $H$ to be a limiting point of $G_n^{-1}/||G^{-1}_n||$. Thanks to the consistency and the unique optimality assumptions, $H$ is only degenerate in direction $a^*$, i.e., $Ha^*=0$ and $||a-a^*||_H = ||a||^2_H>0$.  One can check this by applying the Sherman-Morrison formula to the matrix $K+n a^*{a^*}^T$, where $K$ satisfies $||K||\leq c_pn^p$ for $p>0$. This step ensures that the construction of $\t'$ is valid. On the other hand, as $n\to\infty$,
\begin{align*}
\frac{||a-a^*||^2_{H\bar{G}_nH}}{||a-a^*||^2_{H/||\bar{G}^{-1}_n||}} & = \frac{||a-a^*||^2_{\textcolor{black}{H\bar{G}_n/||\bar{G}_n||}H}}{||a-a^*||^2_{\textcolor{black}{H\bar{G}_n/||\bar{G}_n||}\bar{G}^{-1}_n/||\bar{G}^{-1}_n||}}\to 1\\
\frac{||\bar{G}^{-1}_n||\cdot ||a-a^*||^2_{H}}{||a-a^*||^2_{\bar{G}^{-1}_n}} &= \frac{||a-a^*||^2_{H}}{||a-a^*||^2_{\bar{G}^{-1}_n/||\bar{G}^{-1}_n||}}\to 1\\
\frac{||a||^2_{\bar{G}_n^{-1}}}{||a-a^*||^2_{\bar{G}_n^{-1}}}&\to\frac{||a||^2_{H}}{||a-a^*||^2_{H}} = \frac{||a-a^*||^2_{H}}{||a-a^*||^2_{H}}=1.
\end{align*}
Plugging these back into \eqref{sl:17} yields the desired result. 
\end{proof}
We now see how \eqref{sl:16} implies a lower bound on the asymptotic regret $R_{n,\t}$. First note that
\begin{align*}
R_{n, \t} = \sum_{a\in\mathcal A\setminus a^*}\Delta_a\E[T_a(n)].
\end{align*}
On the other hand, $\bar{G}_n = \sum_{a\in\mathcal A}\E[T_a(n)]aa^T$ needs to satisfy \eqref{sl:16} for every $a\in\mathcal A\setminus a^*$. Equivalently, for $\e>0$ and sufficiently large $n$, $\log n ||a||^2_{\bar{G}_n^{-1}}\leq\Delta_a^2(1+\e)/2$. If we let $\alpha_a(n) = (\log n)^{-1} \E[T_a(n)]$ and $H_n = (\log n)^{-1}\bar{G}_n$. Then, for sufficiently large $n$,
\begin{align*}
\frac{R_{n, \t}}{\log n} \geq\inf_{\alpha}\sum_{a\in\mathcal A\setminus a^*}\Delta_a\alpha_a(n),
\end{align*}
with $\alpha$ satisfying
\begin{align*}
||a||^2_{H_n^{-1}}&\leq \frac{1+\e}{2}\Delta_a^2\ \ \ \ \ \forall a\in\mathcal A\setminus a^*\\
\sum_{a\in\mathcal A}\alpha_a(n) &= \frac{n}{\log n}.  
\end{align*} 
With further relaxation and setting $\e\to 0$ gives
\begin{align*}
\limsup_{n\to\infty}\frac{R_{n,\t}}{\log n}\geq\inf_{\textcolor{blue}{\substack{\alpha\in [0,\infty]^{|\mathcal A|},\  H = \sum_{a\in\mathcal A}\alpha_a aa^T\\ \max_{a\neq a^*}2\Delta_a^{-2}||a||^2_{H^{-1}}\leq 1}}}\sum_{a\in\mathcal A\setminus a^*}\Delta_a\alpha_a. 
\end{align*}
It has been shown in \cite{lattimore2016end} that the above equality is achievable. The first step in such an algorithm is to select a small subset $\mathcal B$ of $\mathcal A$ so that information of all actions is contained inside. This can be done by taking  $\mathcal B$ as a barycentric spanner of $\mathcal A$. Secondly, the algorithm explores $\mathcal B$ to obtain a rough estimate on the suboptimality gaps of all actions. The estimated gaps are then used to approximately solve for the pull counts for each suboptimal actions. If an anomaly is detected that indicates the estimated gaps are inaccurate, the algorithm then switches to a recovery phase where it plays UCB instead. 

As a consequence of Theorem \ref{sl:asym}, consistent optimistic algorithms are not asymptotically optimal. An intuitive explanation is that optimistic algorithms no longer explore actions which are identified suboptimal, and this sometimes takes fewer steps than getting a good estimation for them. A rigorous justification is as follows:

\medskip

\textbf{Example:}\ 
Consider a stochastic linear bandit with $\t=e_1\in\R^2$ and $\mathcal A=\{e_1, e_2, e_3:=(1-\e)e_1+\gamma\e e_2\}$, where $e_{1,2}$ are canonical bases of $\R^2$, $\e>0$ and $\gamma>1$. A consistent UCB algorithm will no longer play $\e_2$ once identifying it is suboptimal (this process should not take long since $e_1\perp e_2$), but later on it will surely hesitate for a long time between $e_1$ and $e_3$ (since $e_1$ and $e_3$ are almost parallel). An asymptotically optimal policy, however, will play $e_2$ longer than seem necessary. This sacrifice will help better decipher the optimal action in $e_1$ and $e_3$. In fact, it can be shown in this case that an asymptotically optimal policy has regret $R_{n,\t}=\mathcal O(\log n)$. However, for the UCB, assuming that the confidence set $\mathcal C_t$ is given by \eqref{sl:13} with $\delta = n^{-1}$, 
\begin{align*}
\t\in\mathcal C_t = \{z\in\R^2, ||z-\hat{\t}_t||^2_{V_{t-1}(0)}\leq c\log n\},
\end{align*}
where $c$ is some constant. Then,
\begin{align*}
\max_{z\in\mathcal C_t}\langle e_2, z\rangle=\max_{z\in\mathcal C_t}\langle e_2, z-\t\rangle\leq 2||e_2||_{V^{-1}_{t-1}}\sqrt{c\log n}< 1\leq \max_{z\in\mathcal C_t}\langle e_1, z\rangle 
\end{align*}
if $||e_2||^2_{V_{t-1}^{-1}}\leq (4c\log n)^{-1}$. A sufficient condition for this is $T_{e_2}(t-1)> 4c\log n$. So conditional on $\mathcal C_t$, $T_{e_2}(t-1)\leq 4c\log n + 1$. Meanwhile, $T_{e_2}(t-1)$ is bounded by $n$ outside $\mathcal C_t$. Therefore,  
\begin{align*}
\E[T_{e_2}(n)]\leq 4c\log n + 2. 
\end{align*}
Equivalently, 
\begin{align*}
\alpha_{e_2}\leq 4c. 
\end{align*}
Yet a consistent policy requires $e_2$ be explored to a given extent. If this is not achieved by simply pulling $e_2$ a number of times,  it must be compensated by pulling other actions which contain information about $e_2$. In our case, this means playing $e_3$ an extra number of times. However, $e_3$ contains only tangential information of $e_2$ so that it must be chosen extensively. Indeed, the consistency assumption implies that 
\begin{align*}
\alpha_{e_3}\e^2+\alpha_{e_2}\geq  2\gamma^2, 
\end{align*}
which further simplifies to $\alpha_{e_3}\geq (2\gamma^2-4c)\e^{-2}$. Hence, if $\gamma>\sqrt{2c}$, then the regret $R_n$ satisfies $R_n = \mathcal O(\e^{-1}\log n)$. Since $\e$ can be arbitrarily small, the regret incurred by the consistent UCB can be any worse than the regret of the best consistent policy. 


\section{Adversarial linear bandits}

\subsection{Problem set-up}
We consider a different type of bandit environment called the adversarial bandit. We will first introduce the basic set-up of the problem as well as an expert version of it, then link them to a more general model called the adversarial linear bandit. Since there is a deep connection between various algorithms for adversarial bandit learning and regularized empirical risk minimization in online learning, we shall put a detailed discussion on the topic to a later stage.  

Let us begin by some definitions. A $k$-armed adversarial bandit is defined as $k$ deterministic sequences $\{x_{ti}\}_{t\in [n]}$, where $i\in [k]$. For simplicity we assume that $x_{ti}\in [0,1]$ representing the loss incurred by playing arm $i$ at $t$. Loss and rewards are transferable by considering $1-x_{ti}$. The key difference, as opposed to the stochastic bandit, is that there is no randomness in the environment. This implies that the learner needs to adopt a non-deterministic policy in order to gain a non-trivial regret, as in the minimax sense. To see it, for any policy $\pi$, the regret in the environment $\{x_{ti}\}_{t\in [n]}$ is given by
\begin{align*}
R_n(\pi) = \E\left[\sum_{t\in [n]}x_{t\pi_t}-\min_{i\in [k]}\sum_{t\in [n]}x_{ti}\right],
\end{align*}
where the expectation is taken with respect to $\pi$. If $\pi$ is deterministic conditional on the history, then we can construct an environment sequentially (our choice of construction at present will not affect the choices of the policy before) by defining $x_{1i}=0$ and for $t\geq 1$, $x_{ti}=\mathbb I_{i=\pi_t}$. By our construction,
\begin{align*}
\sum_{i\in [k]}\sum_{t\in [k]}x_{ti} = n-1,
\end{align*}
implying that $\min_{i\in [k]}\sum_{t\in [n]}x_{ti}\leq\frac{n-1}{k}$. The regret in the above environment is thus lower bounded by $(1-\frac{1}{k})(n-1)$. Hence, the minimax regret for the set of deterministic policies $\mathcal V_d$ is lower bounded by 
\begin{align}
\min_{\pi\in\mathcal V_d}\max_{\{x_{ti}\}\in [0,1]^{nk}}R_n(\pi)\geq \left(1-\frac{1}{k}\right)(n-1).\label{al:ct}
\end{align}
This suggests the necessity to randomize $\pi$ to achieve sub-linear regret in $n$. That is, at step $t$, one selects a distribution $P_t$ on $[k]$ based on the past observations, then samples $\pi_t\sim P_t$. As we will see, with proper choice of $P_t$, the regret grows sub-linearly in $n$.  

A similar problem to the adversarial bandit is bandits with expert advice. In this case, instead of comparing the learner to fixed arms, one compares to the advice of experts. Let $[M]$ denote the group of experts. Every $m\in [M]$ is associated with a sequence of distributions on $[k]$, denoted by $\{E_{tm}\}_{t\in [n]}$. $E_{tm}$ are deterministic and non-interactive\footnote{Since we are adopting a randomized policy, the experts' advice should not depend on the results of the policy.} with $\pi$. The regret is defined by
\begin{align*}
R_n(\pi) = \E\left[\sum_{t\in [n]}x_{t\pi_t}-\min_{m\in [M]}\sum_{t\in [n]}E^T_{tm}x_{t}\right],
\end{align*}
where $x_t = (x_{t1}, \cdots, x_{tk})^T$. The adversarial bandit can be viewed as a special case of the expert-led bandit with $M=k$ and $E_{tm} = \delta_m$. Therefore, a reasonable strategy $\pi$ puts a distribution $P_t$ on the experts at step $t$, and samples $\pi_t$ according to $Q_t:=\sum_{m\in [M]}P_t(m)E_{tm}$. The details of this algorithm will be analyzed together with the Exp3 algorithm in the next section. 

We now introduce the set-up of adversarial linear bandits, which generalizes adversarial bandits to a wider class of environment structures. This is similar to the stochastic linear bandits discussed in the previous chapter. Let $\mathcal A\subset\R^d$ be an action set. For the moment assume that $\mathcal A$ is finite and spans $\R^d$.  An adversarial linear bandit is a sequence of deterministic parameter vectors $\{y_t\}_{t\in [n]}\subset\mathcal A'$, where  
\begin{align*}
\mathcal A' = \left\{x\in\R^d, \sup_{a\in\mathcal A}|\langle x, a\rangle|\leq 1\right\}. 
\end{align*}
Taking $d = k$ and $\mathcal A=\{e_i\}_{i\in [d]}$ recovers the $k$-armed adversarial bandit (with a slightly different loss range). For simplicity, let $a_t$ be the arm chosen in round $t$ under a policy $\pi$. The regret in this case is defined by 
\begin{align}
R_n = \E\left[\sum_{t\in [n]}\langle a_t, y_t\rangle-\min_{a\in\mathcal A}\langle a, y_t\rangle\right].\label{al:1}
\end{align}
In the rest of this chapter we will be focused on discussing algorithms with provable sub-linear bounds for \eqref{al:1}. 


\subsection{Algorithms}

The first algorithm we are going to analyze in this section is called Exp3, which is the abbreviation of \textbf{Exp}onential-weight algorithm for \textbf{Exp}loration and \textbf{Exp}loitation. Exp3 consists of two steps which are summarized as follows: At step $t$, we
\begin{enumerate}
\item estimate the environment parameters $y_s$ before time $t$; and 
\item build a sampling distribution weighted by the estimated cumulative loss for each $a\in\mathcal A$. 
\end{enumerate}
This description tells us that past observations play an important role in future decision making. Based on the estimated cumulative loss, one can naively select the action giving the best performance (which becomes a deterministic policy afterwards!). However, such an approach can be unstable due to the adversarial nature of the problem, as suggested in \eqref{al:ct}. On the other hand, following the leader simply means no more exploration in the future, which may cause trouble if previous exploration is not sufficient. What step (2) does is essentially smoothing out the hard-thresholding decision making process. As we will see later, this is equivalent to minimizing the empirical risk function with appropriate regularization.  

Let $p$ be a probability distribution on $\mathcal A$. We now state the Exp3 algorithm:
\begin{algorithm}
\DontPrintSemicolon
 \KwIn{Action set $\mathcal A$ and mixing parameter $\gamma\in (0,1)$.}
 \KwOut{ $\{a_t\}_{t\in [n]}.$}
 \textbf{Initialization}: $t=1$, $\eta>0$, $a_1\sim\mu$ and $P_1=p$.\\
 \While{$t\leq n$}{
 \begin{itemize}
\item \textcolor{brown}{(Importance-weighted estimator)} $\hat{Y}_t = \langle a_t, y_t\rangle\big(\overbrace{\sum_{a\in\mathcal A}P_{t}(a)aa^T}^{Q(P_t)}\big)^{-1} a_t$; 
\item $P_{t+1}=(1-\gamma)P'_{t+1}+\gamma p$, where $P'_{t+1}(a)\propto e^{-\eta\sum_{s=1}^t\langle a, \hat{Y}_s\rangle}$;
\item $a_{t+1}\sim P_{t+1}$;
\item $t = t +1$. 
\end{itemize}
}
\caption{Exp3 algorithm for adversarial linear bandits.} 
\label{alg:al-Exp3}
\end{algorithm}
\begin{Th}
Suppose that $|\mathcal A|=k$. For carefully chosen $\gamma$ and $\eta$, the regret in Algorithm \ref{alg:al-Exp3} satisfies
\begin{align*}
R_n\leq\sqrt{8(g(p)+d)n\log k},
\end{align*}
where $g(p)=\max_{a\in\mathcal A}||a||^2_{Q^{-1}(p)}$. By Theorem \ref{KW}, there exists some $p$ such that $g(p)=d$. Hence,
\begin{align*}
R_n\leq 4\sqrt{dn\log k}
\end{align*}
\end{Th}
\begin{proof}
The proof is standard for exponential-weighting algorithms. Before we start, it is worth mentioning that exponential weights also arise in regularized empirical risk minimization. However, the algorithm here involves a mixing distribution which is indispensable, as will be detailed in a moment. 

We first note that $\hat{Y}_t$ is an unbiased estimator for $y_t$, therefore $\langle a,y_t\rangle = \E[\langle a, \hat{Y}_t\rangle]$ for fixed $a\in\mathcal A$. The trick is to consider the partition functions given by the exponential weights:
\begin{align*}
W_t = \sum_{a\in\mathcal A}e^{-\eta\sum_{s=1}^t\langle a, \hat{Y}_s\rangle}.
\end{align*}
Define 
\begin{align}
\tilde{P}_t=\frac{P_t-\gamma p}{1-\gamma}.\label{al:2}
\end{align}
Note that for fixed $a\in\mathcal A$, 
\begin{align*}
-\eta\sum_{t\in [n]}\langle a, \hat{Y}_t\rangle\leq\log W_n\leq\sum_{t\in [n]}\log\frac{W_t}{W_{t-1}}+\log k.
\end{align*}
The term $\log\frac{W_t}{W_{t-1}}$ can be estimated explicitly:
\begin{align*}
\log\frac{W_t}{W_{t-1}}&=\log\left(\sum_{a\in\mathcal A}\tilde{P}_t(a)e^{-\eta\langle a, \hat{Y}_t\rangle}\right)\\
&\stackrel{(*)}{\leq}\log\left(\sum_{a\in\mathcal A}\tilde{P}_t(a)(1-\eta\langle a, \hat{Y}_t\rangle+\eta^2 \langle a, \hat{Y}_t\rangle^2)\right)\\
&\leq -\sum_{a\in\mathcal A}\eta\tilde{P}_t(a)\langle a, \hat{Y}_t\rangle+\sum_{a\in\mathcal A}\eta^2 \tilde{P}_t(a)\langle a, \hat{Y}_t\rangle^2,
\end{align*}
where for the $(*)$ we assumed that $\eta\langle a, \hat{Y}_s\rangle\geq -1$ and used that $e^x\leq 1+x+x^2$ when $x\leq 1$. Summing over $t$, replacing $\tilde{P}_t$ by \eqref{al:2} and taking expectation on both sides yields
\begin{align*}
R_n&\leq\frac{\log k}{\eta}+\gamma\E\left[\sum_{t\in [n]}\sum_{a\in\mathcal A}(p(a)-P_t(a))\langle a, \hat{Y}_t\rangle\right]+\frac{1}{1-\gamma}\E\left[\sum_{t\in [n]}\sum_{a\in\mathcal A}\eta P_t(a)\langle a, \hat{Y}_t\rangle^2\right]\\
&\leq \frac{\log k}{\eta}+2\gamma n+\frac{\eta}{1-\gamma}\E\left[\sum_{t\in [n]}\sum_{a\in\mathcal A} P_t(a)\langle a, \hat{Y}_t\rangle^2\right]\\
&= \frac{\log k}{\eta}+2\gamma n+\frac{\eta}{1-\gamma}\E\left[\sum_{t\in [n]}\sum_{a\in\mathcal A} P_t(a)\sum_{b\in\mathcal A}P_t(b)\langle a, Q^{-1}(P_t)\langle b, y_t\rangle b\rangle^2\right]\\
&\stackrel{|\langle b, y_t\rangle|\leq 1}{\leq}\frac{\log k}{\eta}+2\gamma n+\frac{\eta}{1-\gamma}\E\left[\sum_{t\in [n]}\sum_{a\in\mathcal A} P_t(a)\langle a, Q^{-1}(P_t)a\rangle\right]\\
&\leq \frac{\log k}{\eta}+2\gamma n+2\eta nd,
\end{align*}
where the last step follows by assuming $\gamma\leq\frac{1}{2}$. A sufficient condition for $\eta\langle a, \hat{Y}_s\rangle\geq -1$ is
\begin{align*}
\left|\eta\langle a, Q^{-1}(P_t)\langle a_t, y_t\rangle a_t\rangle\right|\leq 1,
\end{align*}
which can be further strengthened as $\left|\eta\langle a, Q^{-1}(P_t)a_t\rangle\right|\leq 1$. Since $Q(P_t)\succcurlyeq \gamma Q(p)$, the above condition holds if $\eta\leq \frac{\gamma}{g(p)}$. Taking the equality and $\gamma=\sqrt{\frac{g^2(p)\log k}{2ng(p)+2nd}}$ finishes the proof.   

<<<<<<< HEAD
\end{proof}


A few more things to add in this section:
\begin{itemize}
\item Extension to the continuous action set (including the $\e$-net argument as well as the continuous exponential weighted sampling). 

\item Explain that the exploration is necessary, which is not in the $k$-armed case. 

\item Give the similar result for expert-led bandits, and a few comments. 

\end{itemize}


\subsection{Online linear optimization}

In this section, we consider the online linear optimization which is closely related to the adversarial bandit problems considered before. From now on we assume that $\mathcal A\subset\R^d$ is convex. Let $a_t$ be the action chosen at time $t$. For fixed environment $\{y_t\}_{t\in [n]}\subset \mathcal A'$ and action $a\in\mathcal A$, the relative regret to $a$ is defined by 
\begin{align*}
R_n(a) = \sum_{t\in [n]}\langle a_t-a, y_t\rangle. 
\end{align*}
The goal of online linear optimization can be roughly stated as finding a strategy so that $\max_{a\in\mathcal A}R_n(a)$ is asymptotically small. This problem is the same as the adversarial linear bandit provided that $y_t$ is explicitly given instead of the marginal $\langle a_t, y_t\rangle$. Nevertheless, such difference can be well addressed by a randomization trick, as observed in Algorithm \ref{alg:al-Exp3}. Therefore, in the rest of the section we will focus on the study of online linear optimization first, then pass the results directly to the adversarial linear bandit set-up. 

A simple strategy for online linear optimization is to \emph{follow the leader} in the past. That is, the action at time $t$ is chosen as the action giving the best cumulative loss up to $t-1$: 
\begin{align*}
a_{t}=\argmin_{a\in\mathcal A}\sum_{s\in [t-1]}\langle a, y_s\rangle.
\end{align*}
This is also referred to as the empirical risk minimization. To see the rationale, note that
\begin{align*}
\sum_{t\in [n]}\langle a_{n+1}, y_t\rangle&=\sum_{t\in [n-1]}\langle a_{n+1}, y_t\rangle+\langle a_{n+1}, y_n\rangle\\
&\geq \sum_{t\in [n-1]}\langle a_{n}, y_t\rangle+\langle a_{n+1}, y_n\rangle\\
&\cdots\\
&\geq \sum_{t\in [n]}\langle a_{t+1}, y_t\rangle,
\end{align*}
which implies that
\begin{align*}
\max_{a\in\mathcal A}R_{n}(a)\leq\sum_{t\in [n]}\langle a_{t+1}-a_t, y_t\rangle.
\end{align*}
This bound would be vacuous unless $a_{t+1}$ does not change dramatically from $a_t$\footnote{This usually depends on the geometry of the action set for which we assume nothing other than convexity.}. Indeed, consider the case when $d=1$, $\mathcal A = [-1,1]$ and $y_1=\frac{1}{2}$ and $y_t = (-1)^{t+1}$ for $t> 1$. It is easy to check that the follow the leader policy leads to $a_t = y_t$ for $t>1$, while the best policy in this case is $a_t = -y_t$ for $t>1$. The issue here is the same as the one we came across in \eqref{al:ct}. To mitigate the potential alternating phenomenon, we consider the regularized version of the follow the leader algorithm (FTRL). Let $F_1, \cdots, F_{n+1}$ be a sequence of convex functions. Define the action at time $t$ as 
\begin{align}
a_t=\argmin_{a\in\mathcal A}\underbrace{\sum_{s\in [t-1]}\langle a, y_s\rangle+F_t(a)}_{\Phi_t(a)}\label{al:3}
\end{align}  
The following theorem provides an upper bound for the relative regret in the FTRL:
\begin{Th}\label{al:FTRL}
Suppose that $a_t$ in \eqref{al:3} is well-defined for all $t\in [n]$. Then, for fixed $a\in\mathcal A$,  
\begin{align*}
R_n(a)&\leq \sum_{t\in [n]}\left(\langle a_t-a_{t+1},y_t\rangle-D_{F_t}(a_{t+1}, a_t)\right)\\
&\ \ \ \ \ +F_{n+1}(a)-F_1(a_1)+\sum_{t=1}^n\left(F_t(a_{t+1})-F_{t+1}(a_{t+1})\right).
\end{align*}
where $D_{F_t}$ is the Bregman divergence defined by $D_{F_t}(x,y)=F(x)-F(y)-\nabla F(y)(x-y)$. Particularly, if $F_t = F/\eta_t$ and $\eta_t$ is decreasing with $\eta_n=\eta_{n+1}$, then
\begin{align*}
R_n(a)=\frac{F(a)-\min_{b\in\mathcal A}F(b)}{\eta_n}+\sum_{t\in [n]}\left(\langle a_t- a_{t+1}, y_t\rangle-\frac{D_F(a_{t+1}, a_t)}{\eta_t}\right).
\end{align*}
\end{Th}
We remark that the divergence terms appearing in the above bounds will penalize the significant difference between $a_t$ and $a_{t+1}$. 
\begin{proof}
We begin by writing 
\begin{align*}
R_n(a)=\sum_{t\in [n]}\langle a_{t+1}-a, y_t\rangle + \sum_{t\in [n]}\langle a_{t}-a_{t+1}, y_t\rangle.
\end{align*}
The first part on the right-hand side can be bounded similarly as in the analysis of the follow-the-leader algorithm. The only difference is that the regularization term allows more accurate estimate for each iterative procedure by the divergence:
\begin{align*}
\sum_{t\in [n]}\langle a, y_t\rangle+F_{n+1}(a)&\geq\sum_{t\in [n]}\langle a_{n+1}, y_t\rangle+F_{n+1}(a_{n+1})\\
& = \sum_{t\in [n-1]}\langle a_{n+1}, y_t\rangle+F_{n}(a_{n+1})+\langle a_{n+1}, y_n\rangle+F_{n+1}(a_{n+1})-F_{n}(a_{n+1})\\
&\geq \sum_{t\in [n-1]}\langle a_{n}, y_t\rangle+F_{n}(a_{n})+D_{F_n}(a_{n+1}, a_n)+\langle a_{n+1}, y_n\rangle+F_{n+1}(a_{n+1})-F_{n}(a_{n+1})\\
&\cdots\\
&\geq \sum_{t\in [n]}\langle a_{t+1}, y_t\rangle+F_1(a_1)+\sum_{t\in [n]}D_{F_t}(a_{t+1}, a_t)+\sum_{t\in [n]}(F_{t+1}(a_{t+1})-F_{t}(a_{t+1})),
\end{align*}
where for the second inequality follows from the first-order optimality of $a_n$:
$$\Phi_n(a_{n+1})-\Phi_n(a_n)\geq D_{\Phi_n}(a_{n+1}, a_n)=D_{F_n}(a_{n+1}, a_n).$$ Reorganizing terms yields the desired result.  
\end{proof}
One could also consider a local version of \eqref{al:3}. Here we assume that $F_t = F/\eta_t$ for some decreasing sequence $\infty=\eta_1>\eta_2\geq\cdots\geq\eta_{n+1}$. 
\begin{align}
a_t&=\argmin_{a\in\mathcal A}\langle a, y_{t-1}\rangle+D_{F_{t}}(a)\nonumber\\
&=\argmin_{a\in\mathcal A}\underbrace{\eta_t\langle a, y_{t-1}\rangle}_{\text{gradient descent}}+\underbrace{D_{F}(a)}_{\text{regularization}}.\label{al:4}
\end{align}
\eqref{al:4} can be interpreted as a two-phase procedure which is often called the mirror descent (primal-dual): first map $a_{t-1}$ to the dual space via the mirror map $F$: $a_{t-1}\mapsto\nabla F(a_{t-1})$, take the gradient descent with step length $\eta_t$, and pull back to the point $\tilde{a}_t$ in the domain of $F$. Then obtain $a_t$ from $\tilde{a}_t$ by taking the Bregman divergence projection to the action set $\mathcal A$. The two steps can be summarized as follows under appropriate domain conditions, 
\begin{align*}
\tilde{a}_t &= \argmin_{a\in\text{Dom}(F)}\eta_t\langle a, y_{t-1}\rangle + D_F(a, a_{t-1})\\
a_t &= \text{Proj}_{D_F, \mathcal A}(\tilde{a}_{t}).
\end{align*}
A similar bound for the regret holds true as in the case of FTRL:
\begin{Th}\label{al:MR}
Suppose that $a_t$ in \eqref{al:4} is well-defined for all $t\in [n]$. Then, for fixed $a\in\mathcal A$,  
\begin{align*}
R_n(a)&=\sum_{t\in [n]}\frac{D_F(a_t, \tilde{a}_{t+1})}{\eta_{t+1}}+\sum_{t\in [n]}\frac{D_F(a,a_t)-D(a,\tilde{a}_{t+1})}{\eta_{t+1}}\\
&\leq \sum_{t\in [n]}\frac{D_F(a_t, \tilde{a}_{t+1})}{\eta_{t+1}}+\sum_{t\in [n]}D_F(a,a_t)\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_t}\right).
\end{align*}
\begin{proof}
The first-order optimality of $\tilde{a}_{t+1}$ implies that 
\begin{align*}
y_t = \frac{1}{\eta_{t+1}}\left(\nabla F(a_t)-\nabla F(\tilde{a}_{t+1})\right).
\end{align*}
Plugging into the definition of $R_n(a)$, 
\begin{align*}
R_n(a)=\sum_{t\in [n]}\langle a_t-a, y_t\rangle &= \sum_{t\in [n]}\frac{1}{\eta_{t+1}}\langle a_t-a, \nabla F(a_t)-\nabla F(\tilde{a}_{t+1})\rangle\\
& = \sum_{t\in [n]}\frac{1}{\eta_{t+1}}\left(D_F(a_t, \tilde{a}_{t+1})+D_F(a, a_t)-D_F(a, \tilde{a}_{t+1})\right)\\
& \leq \sum_{t\in [n]}\frac{D_F(a_t, \tilde{a}_{t+1})}{\eta_{t+1}}+\sum_{t\in [n]}\frac{D_F(a, a_t)-D_F(a, a_{t+1})}{\eta_{t+1}},
\end{align*}
where for the last step we used that $D_F(a, a_{t+1})\leq D_F(a, \tilde{a}_{t+1})$. Rearranging terms leads to the desired result. 
\end{proof}
\end{Th}
Results in Theorem \ref{al:FTRL} and \ref{al:MR} can be easily modified to apply to the randomized versions of the algorithms. Indeed, as in adversarial linear bandits, suppose that one can estimate $y_s$ by some unbiased estimator $\hat{Y}_{s}$.  The decision at $t$ should be $a_t=\argmin_{a\in\mathcal A}\sum_{s\in [t-1]}\langle a, \hat{Y}_s\rangle+F_t(a):=a_t^*$ or $a_{t}=\argmin_{a\in\mathcal A}\langle a, \hat{Y}_{t-1}\rangle+D_{F_{t}}(a):=a^{**}_t$. However, conditional on history, both algorithms will lead to fixed choices in the future so that bias cannot be resolved. A possible way to get around this issue is to sample $a_t$ from some distribution $P_t$\footnote{One has enough freedom to design such a distribution, which is itself an art.} on $\mathcal A$ with mean $a_t^*$ or $a^{**}_t$. This leads to the following algorithm:
\begin{algorithm}
\DontPrintSemicolon
 \KwIn{Action set $\mathcal A$, regularization $F(a)$ and learning rate $\{\eta_t\}_{t\in [n]}$.}
 \KwOut{ $\{a_t\}_{t\in [n]}.$}
 \textbf{Initialization}: $t=1$, $a_1=\argmin_{a\in\mathcal A}F(a)$.\\
 \While{$t\leq n$}{
 \begin{itemize}
\item Estimate $y_t$ by an unbiased estimator $\hat{Y}_t$, say the importance-weighted estimator; 
\item \begin{align*}
\bar{a}_{t+1}&=\argmin_{a\in\mathcal A}\eta_{t+1}\sum_{s\in [t]}\langle a, \hat{Y}_s\rangle+F(a)\ \ \text{(FTRL)}\\
\bar{a}_{t+1}&=\argmin_{a\in\mathcal A}\eta_{t+1}\langle a, \hat{Y}_{t-1}\rangle+D_{F}(a)\ \ \text{(Mirror descent)}
\end{align*}
\item Sample $a_{t+1}$ from $P_{t+1}$, where $P_{t+1}$ is some distribution on $\mathcal A$ with mean $\bar{a}_{t+1}$;
\item $t = t +1$. 
\end{itemize}
}
\caption{FTRL/Mirror descent algorithms for adversarial linear bandits.} 
\label{alg:al-Exp3r}
\end{algorithm}
The regret bound given by Algorithm \ref{alg:al-Exp3r} follows directly from Theorem \ref{al:FTRL} and \ref{al:MR}. Take the FTRL for instance. For each $t$, first conditional on $\bar{a}_t$ and then averaging over $P_t$, 
\begin{align}
 \sum_{t\in [n]}\E[\langle \bar{a}_t-a, \hat{Y}_t\rangle]= \sum_{t\in [n]}\E[\langle \bar{a}_t-a, y_t\rangle]= \sum_{t\in [n]}\E[\langle a_t-a, y_t\rangle]=R_n.
\end{align}
Therefore,
\begin{align*}
R_n\leq\frac{\max_{a,b\in\mathcal A}F(a)-F(b)}{\eta_n}+\E\left[\sum_{t\in [n]}\underbrace{\left(\langle \bar{a}_t- \bar{a}_{t+1}, y_t\rangle-\frac{D_F(\bar{a}_{t+1}, \bar{a}_t)}{\eta_t}\right)}_{\frac{\eta_t}{2}||y_t||_{H_t^{-1}}^2}\right].
\end{align*} 
where $H_t$ is the Hessian of $F$ at some point $\xi_t$ between $\bar{a}_t$ and $\bar{a}_{t+1}$.

We now see a few examples:

\begin{Ex}[$k$-armed adversarial bandits]\label{al:ex1}
Let $d=k and \mathcal A = \text{conv}(e_i)_{i\in [k]}$\footnote{It is easy to check that maximizing the relative loss over $\{e_i\}$ is equivalent to maximizing over $\text{conv}(e_i)_{i\in [k]}$.}. Set $F(a)$ as the negentropy $\sum_{i\in [k]}a^{(i)}(\log a^{(i)}-1)$, where $a=(a^{(i)})_{i\in [k]}$. It is easy to check that $a_1=(\frac{1}{k}, \cdots, \frac{1}{k})^T$. For $x\in\mathcal A$ and $y\in\R_+^k$, 
\begin{align*}
D_F(x,y) &= \sum_{i\in [k]}x^{(i)}(\log x^{(i)}-1)-\sum_{i\in [k]}y^{(i)}(\log y^{(i)}-1)-\sum_{i\in [k]}\log y^{(i)}(x^{(i)}-y^{(i)})\\
& = \sum_{i\in [k]}x^{(i)}\log \frac{x^{(i)}}{y^{(i)}}+\text{terms of}\ y.
\end{align*}
Therefore, $\text{Proj}_{F, \mathcal A}(y) = \frac{y}{||y||_1}$. From direct computation, we find that
\begin{align}
a_t^{(i)} = \frac{e^{-\eta_t\sum_{s\in [t-1]} y_s^{(i)}}}{\sum_{i\in [k]}e^{-\eta_t\sum_{s\in [t-1]} y_s^{(i)}}}. \label{al:5}
\end{align}
Replacing $y_i$ by the importance-weighted estimator and and choosing $P_t$ as a distribution on $\{e_i\}_{i\in [k]}$ with $P_t(e_i)=a_t^{(i)}$ ($A_t\sim P_t$) recovers the Exp3 algorithm for $k$-armed bandits. 
Using the fact that $F\in [1,\log k+1] $, $\nabla^{-2} F(a) = \text{diag}(a)$ and $\hat{Y}_t=\frac{\langle y_t, e_{A_t}\rangle}{P_{A_t}}e_{A_t}$,
\begin{align*}
R_n\leq\frac{\log k}{\eta_n}+\frac{k}{2}\sum_{t\in [n]}\eta_t.
\end{align*}
Taking $\eta_t = \sqrt{\log k/kt}$ (adaptive learning rates) yields $R_n\leq 2\sqrt{nk\log k}$.
\end{Ex}

\begin{Ex}[The log barrier and first order bounds]
This example is taken from an exercise in \cite{lattimore2018bandit}, which is adapted from the work \cite{wei2018more}. It has exactly the same context as in the Example \ref{al:ex1} except with a different regularization function: $F(a)=-\sum_{i\in [k]}\log a^{(i)}$. This gives a different weight matrix $\nabla^{-2} F(x) = \text{diag}(x^2)$. Therefore,
\begin{align*}
R_n(a)\leq\frac{F(a)-k\log k}{\eta_n}+\frac{1}{2}\sum_{t\in [n]}\E[\eta_t\langle y_t, e_{A_t}\rangle^2].
\end{align*}
When maximizing over $a$, the bound becomes useless since $F(a)$ blows up when $a$ approaches the corners of $\mathcal A$. We consider a truncated version of $\mathcal A$ and use the boundedness assumption of the bandit to achieve a more reasonable bound for the regret. For convenience let arm $1$ correspond to the optimal action. Then, 
\begin{align*}
R_n&\leq\min_{a\in\mathcal A\cap\{\min_{i\in [k]} a^{(i)}\geq\frac{1}{n}\}}\sum_{t\in [n]}\langle a-e_1, y_t\rangle +\max_{a\in\mathcal A\cap\{\min_{i\in [k]} a^{(i)}\geq\frac{1}{n}\}} R_n(a)\\
&\leq k-1+\frac{k\log\frac{n}{k}}{\eta_n}+\frac{1}{2}\sum_{t\in [n]}\E[\eta_t\langle y_t, e_{A_t}\rangle^2].
\end{align*}
Taking 
\begin{align*}
\eta_t\footnote{\text{Even though the learning rates are adaptive, they still depend on the horizon $n$.}}
 = \sqrt{\frac{k\log (\frac{n\wedge k}{k})}{1+\sum_{s\in [t-1]}\langle y_s, e_{A_s}\rangle^2}}
\end{align*} 
yields
\begin{align*}
R_n &\leq k-1+2\sqrt{k\left(1+\E\left[\sum_{t\in [n]}\langle a_t, e_{A_t}\rangle^2\right]\right)\log\left(\frac{n\wedge k}{k}\right)}\\
&\leq k-1+2\sqrt{k\left(1+R_n+\min_{i\in [k]}\sum_{t\in [n]}\langle a_t, e_i\rangle\right)\log\left(\frac{n\wedge k}{k}\right)},
\end{align*}
which further simplifies to $R_n=\mathcal O\left(\sqrt{k\min_{i\in [k]}\sum_{t\in [n]}\langle a_t, e_i\rangle\log\left(\frac{n\wedge k}{k}\right)}\right)$. 

\end{Ex}
 


\subsection{Connections to stochastic linear bandits}
 
%=======
%>>>>>>> baf27d0de4aef4e5eb782743d2f58d27984505ef
\printbibliography

 
 
\end{document}
